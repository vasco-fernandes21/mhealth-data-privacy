# ============================================================================
# WESAD SPECIFIC CONFIGURATION
# ============================================================================
# Otimizado para WESAD - sobrepõe training_defaults

dataset:
  name: "wesad"
  path: "data/processed/wesad"
  n_channels: 14
  n_classes: 2
  sequence_length: 1920
  class_names:
    - "non-stress"
    - "stress"

model:
  architecture: "lstm"
  input_projection: 128
  lstm_units: 56
  lstm_layers: 1
  lstm_bidirectional: true
  dropout: 0.48
  dense_layers: [112, 56]
  activation: "relu"

training:
  epochs: 100
  batch_size: 12
  learning_rate: 0.0008
  optimizer: "adamw"
  weight_decay: 0.0003
  loss: "focal_loss"
  focal_alpha: 0.25
  focal_gamma: 2.0
  label_smoothing: 0.13
  
  # LR Scheduler - OTIMIZADO PARA WESAD
  lr_scheduler: "warmup_cosine"
  warmup_epochs: 3
  
  early_stopping: true
  early_stopping_patience: 18  # WESAD-específico
  
  gradient_clipping: true
  gradient_clip_norm: 1.0
  
  num_workers: 0
  verbose: true