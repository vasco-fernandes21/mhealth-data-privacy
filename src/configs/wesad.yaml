# ============================================================================
# WESAD DATASET CONFIGURATION
# ============================================================================
# Optimized for baseline (no privacy) - standard ML practices

dataset:
  name: "wesad"
  path: "data/processed/wesad"
  n_channels: 14
  n_classes: 2
  sequence_length: 1920  # 60 seconds @ 32 Hz
  class_names:
    - "non-stress"
    - "stress"

model:
  architecture: "lstm"
  input_projection: 128
  lstm_units: 64
  lstm_layers: 2
  lstm_bidirectional: true
  dropout: 0.3
  dense_layers: [64, 32]
  activation: "relu"
  use_group_norm: true
  group_norm_groups: 8

training:
  # Dataset-specific overrides
  batch_size: 64
  learning_rate: 0.0005
  # Other settings inherited from training_defaults.yaml
  # - epochs: 100
  # - optimizer: "adam"
  # - weight_decay: 1e-4
  # - loss: "cross_entropy"
  # - label_smoothing: 0.05
  # - use_class_weights: false
  # - lr_scheduler: "none"
  # - gradient_clipping: true
  # - gradient_clip_norm: 1.0
  # - early_stopping_patience: 10