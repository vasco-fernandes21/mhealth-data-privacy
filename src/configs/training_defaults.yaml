# ============================================================================
# TRAINING DEFAULT CONFIGURATION
# ============================================================================
# Common defaults for all training scenarios
# Optimized for baseline (no privacy) with standard ML practices

training:
  # Optimization
  epochs: 100
  learning_rate: 0.0005
  optimizer: "adam"
  weight_decay: 1e-4
  
  # Loss function
  loss: "cross_entropy"
  label_smoothing: 0.05
  use_class_weights: false  # Disabled (hurt WESAD performance)
  
  # Learning rate scheduling
  lr_scheduler: "none"  # Fixed LR for fair comparison with DP
  
  # Early stopping
  early_stopping: true
  early_stopping_patience: 10
  
  # Gradient clipping (IMPORTANT: compatible with DP later)
  gradient_clipping: true
  gradient_clip_norm: 1.0
  
  # Data loading
  num_workers: 4
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 2
  drop_last: false
  
  # Logging
  log_every_n_batches: 10
  verbose: true

# Device configuration
device:
  type: "auto"  # "cuda", "cpu", or "auto"
  benchmark: true  # CUDA optimization