# ============================================================================
# PRIVACY DEFAULT CONFIGURATION
# ============================================================================
# Configuration for Differential Privacy and Federated Learning
# 
# Estratégia: Apenas override FL/DP params, herda tudo do baseline

differential_privacy:
  enabled: false
  
  # Privacy budget - REALISTA
  target_epsilon: 8.0
  target_delta: 1e-5
  
  # Gradient clipping
  max_grad_norm: 1.0
  
  # Noise
  noise_multiplier: 0.5    # Reduzido (era 0.9)
  poisson_sampling: true
  
  accounting_method: "rdp"
  secure_rng: true
  grad_sample_mode: "hooks"

# ============= FL-SPECIFIC OVERRIDES =============
# Apenas parâmetros que divergem do baseline
training:
  # Para FL: batch size pode ser diferente
  batch_size: 12           # Match baseline
  learning_rate: 0.0008    # Match baseline
  
  # Para DP: seria diferente
  # batch_size: 64         # DP: aumenta
  # learning_rate: 0.001   # DP: aumenta
  
  # Scheduler - Keep baseline
  lr_scheduler: "warmup_cosine"
  warmup_epochs: 3

federated_learning:
  enabled: false
  
  n_clients: 10
  samples_per_client: 100
  
  # Communication - FL-ESPECÍFICO
  global_rounds: 100
  local_epochs: 5
  local_batch_size: 12
  
  aggregation_method: "fedavg"
  trimming_fraction: 0.1
  
  client_dp: false
  client_noise_multiplier: 0.5
  
  eval_every_n_rounds: 1

federated_learning_dp:
  enabled: false
  
  target_epsilon: 5.0
  target_delta: 1e-5
  
  server_noise: true
  server_noise_multiplier: 0.5
  
  client_noise: true
  client_noise_multiplier: 0.3
  
  global_rounds: 100
  local_epochs: 1