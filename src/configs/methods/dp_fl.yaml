method: "dp_fl"
description: "Federated Learning + Differential Privacy"

differential_privacy:
  enabled: true
  target_epsilon: [1.0, 3.0, 8.0]
  target_delta: 1e-5
  client_noise_multiplier: 0.3
  server_noise_multiplier: 0.5
  max_grad_norm: 1.0

federated_learning:
  enabled: true
  n_clients: 10
  global_rounds: 100
  local_epochs: 1  # Menos com DP
  local_batch_size: 32

training:
  optimizer: "sgd"
  learning_rate: 0.001
  epochs: 150
  early_stopping_patience: 40