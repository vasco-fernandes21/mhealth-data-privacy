{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# WESAD PyTorch Training - Google Colab\n",
        "\n",
        "This notebook trains PyTorch models for stress detection using the WESAD (Wearable Stress and Affect Detection) dataset.\n",
        "\n",
        "## Features:\n",
        "- Baseline LSTM model for binary stress classification\n",
        "- Differential Privacy (DP-SGD) training option\n",
        "- Efficient data loading for physiological signals\n",
        "- Clean logs during training (no emojis)\n",
        "- Early stopping and learning rate scheduling\n",
        "- Memory and speed optimizations\n",
        "- Automatic hardware acceleration (CUDA > MPS > CPU)\n",
        "\n",
        "## Requirements:\n",
        "- Google Colab with GPU enabled\n",
        "- Data in Google Drive: `mydrive/mhealth-data/data/processed/wesad/`\n",
        "\n",
        "## Training Options:\n",
        "1. **Baseline**: Standard LSTM model (~80% accuracy)\n",
        "2. **Differential Privacy**: DP-SGD with privacy guarantees\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Initial Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install necessary dependencies\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install numpy pandas scikit-learn matplotlib seaborn\n",
        "!pip install mne pyedflib opacus\n",
        "\n",
        "# Clone the repository\n",
        "!git clone https://github.com/vasco-fernandes21/mhealth-data-privacy.git\n",
        "import sys\n",
        "sys.path.append('/content/mhealth-data-privacy')\n",
        "\n",
        "print(\"Dependencies installed and repository cloned\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check if data exists\n",
        "import os\n",
        "data_path = '/content/drive/MyDrive/mhealth-data/data/processed/wesad'\n",
        "\n",
        "if os.path.exists(data_path):\n",
        "    print(f\"Data found at: {data_path}\")\n",
        "\n",
        "    # List files\n",
        "    files = os.listdir(data_path)\n",
        "    print(f\"Available files: {files}\")\n",
        "\n",
        "    # Check file sizes\n",
        "    for file in ['X_train.npy', 'y_train.npy', 'X_val.npy', 'y_val.npy', 'X_test.npy', 'y_test.npy']:\n",
        "        if file in files:\n",
        "            size = os.path.getsize(os.path.join(data_path, file))\n",
        "            print(f\"  {file}: {size / (1024*1024):.1f} MB\")\n",
        "        else:\n",
        "            print(f\"  {file} not found\")\n",
        "else:\n",
        "    print(f\"Data not found at: {data_path}\")\n",
        "    print(\"Make sure the data is in the correct path on Google Drive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Training\n",
        "\n",
        "Choose your training option:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option A: Baseline LSTM Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run baseline training using the repository script\n",
        "import os, shutil\n",
        "\n",
        "repo_data_dir = '/content/mhealth-data-privacy/data/processed/wesad'\n",
        "drive_data_dir = '/content/drive/MyDrive/mhealth-data/data/processed/wesad'\n",
        "models_dir = '/content/mhealth-data-privacy/models/wesad/baseline'\n",
        "results_dir = '/content/mhealth-data-privacy/results/wesad/baseline'\n",
        "\n",
        "# Ensure output directories\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Create symbolic link from Drive data to expected path\n",
        "os.makedirs('/content/mhealth-data-privacy/data/processed', exist_ok=True)\n",
        "if os.path.islink(repo_data_dir) or os.path.exists(repo_data_dir):\n",
        "    try:\n",
        "        if os.path.islink(repo_data_dir):\n",
        "            os.unlink(repo_data_dir)\n",
        "        else:\n",
        "            shutil.rmtree(repo_data_dir)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning when removing old destination: {e}\")\n",
        "\n",
        "# Create symlink\n",
        "!ln -sf \"$drive_data_dir\" \"$repo_data_dir\"\n",
        "print(f\"Data referenced via symlink: {repo_data_dir} -> {drive_data_dir}\")\n",
        "\n",
        "print(f\"Starting baseline LSTM training with data from: {drive_data_dir}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Run the baseline training script\n",
        "!python /content/mhealth-data-privacy/src/train/wesad/train_baseline.py\n",
        "\n",
        "print(\"Baseline training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option B: Differential Privacy Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run differential privacy training\n",
        "import os, shutil\n",
        "\n",
        "repo_data_dir = '/content/mhealth-data-privacy/data/processed/wesad'\n",
        "drive_data_dir = '/content/drive/MyDrive/mhealth-data/data/processed/wesad'\n",
        "models_dir = '/content/mhealth-data-privacy/models/wesad/dp'\n",
        "results_dir = '/content/mhealth-data-privacy/results/wesad/dp'\n",
        "\n",
        "# Ensure output directories\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Create symbolic link from Drive data to expected path\n",
        "os.makedirs('/content/mhealth-data-privacy/data/processed', exist_ok=True)\n",
        "if os.path.islink(repo_data_dir) or os.path.exists(repo_data_dir):\n",
        "    try:\n",
        "        if os.path.islink(repo_data_dir):\n",
        "            os.unlink(repo_data_dir)\n",
        "        else:\n",
        "            shutil.rmtree(repo_data_dir)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning when removing old destination: {e}\")\n",
        "\n",
        "# Create symlink\n",
        "!ln -sf \"$drive_data_dir\" \"$repo_data_dir\"\n",
        "print(f\"Data referenced via symlink: {repo_data_dir} -> {drive_data_dir}\")\n",
        "\n",
        "print(f\"Starting differential privacy training with data from: {drive_data_dir}\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Using DPLSTM for Opacus compatibility\")\n",
        "\n",
        "# Run the differential privacy training script\n",
        "!python /content/mhealth-data-privacy/src/train/wesad/differential_privacy/train_dp.py\n",
        "\n",
        "print(\"Differential privacy training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and analyze results\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Check which results are available\n",
        "baseline_results_path = '/content/mhealth-data-privacy/models/wesad/baseline/results_wesad_baseline.json'\n",
        "dp_results_path = '/content/mhealth-data-privacy/models/wesad/dp/results_wesad_dp.json'\n",
        "\n",
        "results_to_show = []\n",
        "\n",
        "if os.path.exists(baseline_results_path):\n",
        "    with open(baseline_results_path, 'r') as f:\n",
        "        baseline_results = json.load(f)\n",
        "    results_to_show.append(('Baseline LSTM', baseline_results))\n",
        "\n",
        "if os.path.exists(dp_results_path):\n",
        "    with open(dp_results_path, 'r') as f:\n",
        "        dp_results = json.load(f)\n",
        "    results_to_show.append(('Differential Privacy', dp_results))\n",
        "\n",
        "if results_to_show:\n",
        "    print(\"FINAL RESULTS:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for model_name, results in results_to_show:\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        print(f\"   Accuracy:  {results['accuracy']:.4f}\")\n",
        "        print(f\"   Precision: {results['precision']:.4f}\")\n",
        "        print(f\"   Recall:    {results['recall']:.4f}\")\n",
        "        print(f\"   F1-Score:  {results['f1_score']:.4f}\")\n",
        "        \n",
        "        if 'epsilon' in results:\n",
        "            print(f\"   Privacy (ε): {results['epsilon']:.4f}\")\n",
        "        \n",
        "        if 'training_time' in results:\n",
        "            print(f\"   Training Time: {results['training_time']:.1f}s\")\n",
        "\n",
        "    # Plot confusion matrices\n",
        "    fig, axes = plt.subplots(1, len(results_to_show), figsize=(6*len(results_to_show), 5))\n",
        "    if len(results_to_show) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for i, (model_name, results) in enumerate(results_to_show):\n",
        "        cm = np.array(results['confusion_matrix'])\n",
        "        class_names = results['class_names']\n",
        "        \n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=class_names, yticklabels=class_names, ax=axes[i])\n",
        "        axes[i].set_title(f'Confusion Matrix - {model_name}')\n",
        "        axes[i].set_xlabel('Predicted')\n",
        "        axes[i].set_ylabel('Actual')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"No results found\")\n",
        "    print(\"Run the training cells first\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Results Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run baseline training 5 times with different seeds\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Configuration\n",
        "num_runs = 5\n",
        "seeds = [42, 123, 456, 789, 999]\n",
        "results_baseline = []\n",
        "\n",
        "# Setup directories\n",
        "repo_data_dir = '/content/mhealth-data-privacy/data/processed/wesad'\n",
        "drive_data_dir = '/content/drive/MyDrive/mhealth-data/data/processed/wesad'\n",
        "base_models_dir = '/content/mhealth-data-privacy/models/wesad'\n",
        "base_results_dir = '/content/mhealth-data-privacy/results/wesad'\n",
        "\n",
        "# Create symlink once\n",
        "os.makedirs('/content/mhealth-data-privacy/data/processed', exist_ok=True)\n",
        "if os.path.islink(repo_data_dir) or os.path.exists(repo_data_dir):\n",
        "    try:\n",
        "        if os.path.islink(repo_data_dir):\n",
        "            os.unlink(repo_data_dir)\n",
        "        else:\n",
        "            shutil.rmtree(repo_data_dir)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: {e}\")\n",
        "\n",
        "!ln -sf \"$drive_data_dir\" \"$repo_data_dir\"\n",
        "print(f\"Data linked: {repo_data_dir} -> {drive_data_dir}\\n\")\n",
        "\n",
        "print(\"Starting 5 baseline runs...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, seed in enumerate(seeds):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"RUN {i+1}/5 - SEED {seed}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Create run-specific directories\n",
        "    run_dir = f'baseline_run{i+1}'\n",
        "    models_dir = f'{base_models_dir}/{run_dir}'\n",
        "    results_dir = f'{base_results_dir}/{run_dir}'\n",
        "    os.makedirs(models_dir, exist_ok=True)\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "    \n",
        "    # Modify seed in training script by setting environment variable\n",
        "    os.environ['TRAIN_SEED'] = str(seed)\n",
        "    os.environ['MODEL_DIR'] = models_dir\n",
        "    os.environ['RESULTS_DIR'] = results_dir\n",
        "    \n",
        "    # Run training\n",
        "    !python /content/mhealth-data-privacy/src/train/wesad/train_baseline.py\n",
        "    \n",
        "    # Load results\n",
        "    results_path = f'{models_dir}/results_wesad_baseline.json'\n",
        "    if os.path.exists(results_path):\n",
        "        with open(results_path, 'r') as f:\n",
        "            run_results = json.load(f)\n",
        "        \n",
        "        results_baseline.append({\n",
        "            'run': i+1,\n",
        "            'seed': seed,\n",
        "            'accuracy': run_results['accuracy'],\n",
        "            'f1_score': run_results['f1_score'],\n",
        "            'precision': run_results['precision'],\n",
        "            'recall': run_results['recall'],\n",
        "            'confusion_matrix': run_results['confusion_matrix'],\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        })\n",
        "        \n",
        "        print(f\"\\nRun {i+1} Results: Accuracy={run_results['accuracy']:.4f}, F1={run_results['f1_score']:.4f}\")\n",
        "    else:\n",
        "        print(f\"Warning: Results not found for run {i+1}\")\n",
        "\n",
        "# Calculate statistics\n",
        "if results_baseline:\n",
        "    accuracies = [r['accuracy'] for r in results_baseline]\n",
        "    f1_scores = [r['f1_score'] for r in results_baseline]\n",
        "    precisions = [r['precision'] for r in results_baseline]\n",
        "    recalls = [r['recall'] for r in results_baseline]\n",
        "    \n",
        "    stats = {\n",
        "        'mean_accuracy': float(np.mean(accuracies)),\n",
        "        'std_accuracy': float(np.std(accuracies)),\n",
        "        'mean_f1': float(np.mean(f1_scores)),\n",
        "        'std_f1': float(np.std(f1_scores)),\n",
        "        'mean_precision': float(np.mean(precisions)),\n",
        "        'std_precision': float(np.std(precisions)),\n",
        "        'mean_recall': float(np.mean(recalls)),\n",
        "        'std_recall': float(np.std(recalls)),\n",
        "        'runs': results_baseline\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"BASELINE STATISTICS (5 RUNS)\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Accuracy:  {stats['mean_accuracy']:.4f} ± {stats['std_accuracy']:.4f}\")\n",
        "    print(f\"Precision: {stats['mean_precision']:.4f} ± {stats['std_precision']:.4f}\")\n",
        "    print(f\"Recall:    {stats['mean_recall']:.4f} ± {stats['std_recall']:.4f}\")\n",
        "    print(f\"F1-Score:  {stats['mean_f1']:.4f} ± {stats['std_f1']:.4f}\")\n",
        "    \n",
        "    # Save results\n",
        "    os.makedirs(f'{base_results_dir}/baseline', exist_ok=True)\n",
        "    with open(f'{base_results_dir}/baseline/baseline_5runs.json', 'w') as f:\n",
        "        json.dump(stats, f, indent=2)\n",
        "    \n",
        "    print(f\"\\nResults saved to: {base_results_dir}/baseline/baseline_5runs.json\")\n",
        "else:\n",
        "    print(\"No results collected!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tips and Troubleshooting\n",
        "\n",
        "### Common Issues:\n",
        "\n",
        "1. **Data not found:**\n",
        "   - Check if path `mydrive/mhealth-data/data/processed/wesad/` is correct\n",
        "   - Ensure all `.npy` files are present\n",
        "\n",
        "2. **GPU not available:**\n",
        "   - Go to Runtime → Change runtime type → Hardware accelerator → GPU\n",
        "   - Model will work on CPU but will be slower\n",
        "   - Automatic hardware detection: CUDA > MPS > CPU\n",
        "\n",
        "3. **Insufficient memory:**\n",
        "   - WESAD dataset is smaller than Sleep-EDF (~10MB for training)\n",
        "   - LSTM model has ~200K parameters\n",
        "   - Consider smaller batch_size if needed\n",
        "\n",
        "4. **Long training time:**\n",
        "   - First epoch always takes longer (initial loading)\n",
        "   - Logs show progress every 3 batches for DP training\n",
        "   - Early stopping after 10 epochs without improvement\n",
        "\n",
        "5. **Opacus LSTM compatibility:**\n",
        "   - Fixed: Uses DPLSTM instead of nn.LSTM for DP training\n",
        "   - DPLSTM is a drop-in replacement compatible with Opacus\n",
        "   - **Why DPLSTM?** Opacus requires special DP-compatible layers because nn.LSTM uses internal modules that break gradient sampling hooks needed for differential privacy\n",
        "\n",
        "### Model Architecture:\n",
        "- **LSTM**: Bidirectional with 2 layers (64 hidden units)\n",
        "- **Dense**: 2 fully connected layers for classification\n",
        "- **Total**: ~200K parameters\n",
        "\n",
        "### Resources Used:\n",
        "- **GPU**: Recommended for fast training\n",
        "- **RAM**: ~2GB for dataset + model\n",
        "- **Disk**: ~100MB for code + results\n",
        "\n",
        "### Next Steps:\n",
        "1. Compare baseline vs differential privacy performance\n",
        "2. Analyze privacy-utility trade-offs\n",
        "3. Test with other physiological datasets\n",
        "\n",
        "---\n",
        "\n",
        "**Notebook created for SIDM - MHealth Data Privacy project**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
