============================================================
✓ Using cached sleep-edf
Data: train=(313922, 24) val=(66753, 24) test=(72330, 24)
Model: 12,165 params

DP Training
  Noise multiplier: 0.6
  Max grad norm: 1.0
  Delta: 1e-05
  Epochs: 60

Epoch 01: loss=0.9171 acc=0.7606 | val_loss=0.8229 val_acc=0.8143 | ε=1.2803
Epoch 02: loss=0.8251 acc=0.8070 | val_loss=0.7794 val_acc=0.8326 | ε=1.5047
Epoch 03: loss=0.8012 acc=0.8208 | val_loss=0.7743 val_acc=0.8380 | ε=1.6520
Epoch 04: loss=0.7917 acc=0.8250 | val_loss=0.7671 val_acc=0.8397 | ε=1.7666
Epoch 05: loss=0.7851 acc=0.8275 | val_loss=0.7712 val_acc=0.8427 | ε=1.8630
Epoch 06: loss=0.7715 acc=0.8314 | val_loss=0.7619 val_acc=0.8442 | ε=1.9479
Epoch 07: loss=0.7651 acc=0.8326 | val_loss=0.7478 val_acc=0.8444 | ε=2.0247
Epoch 08: loss=0.7528 acc=0.8349 | val_loss=0.7503 val_acc=0.8461 | ε=2.0956
Epoch 09: loss=0.7542 acc=0.8364 | val_loss=0.7418 val_acc=0.8469 | ε=2.1619
Epoch 10: loss=0.7509 acc=0.8360 | val_loss=0.7584 val_acc=0.8470 | ε=2.2246
Epoch 11: loss=0.7451 acc=0.8372 | val_loss=0.7571 val_acc=0.8470 | ε=2.2844
Epoch 12: loss=0.7412 acc=0.8386 | val_loss=0.7509 val_acc=0.8487 | ε=2.3417
Epoch 13: loss=0.7359 acc=0.8396 | val_loss=0.7371 val_acc=0.8484 | ε=2.3970
Epoch 14: loss=0.7375 acc=0.8396 | val_loss=0.7275 val_acc=0.8497 | ε=2.4505
Epoch 15: loss=0.7294 acc=0.8402 | val_loss=0.7366 val_acc=0.8500 | ε=2.5025
Epoch 16: loss=0.7362 acc=0.8401 | val_loss=0.7376 val_acc=0.8517 | ε=2.5531
Epoch 17: loss=0.7304 acc=0.8413 | val_loss=0.7217 val_acc=0.8522 | ε=2.6026
Epoch 18: loss=0.7365 acc=0.8405 | val_loss=0.7365 val_acc=0.8524 | ε=2.6510
Epoch 19: loss=0.7327 acc=0.8415 | val_loss=0.7220 val_acc=0.8522 | ε=2.6984
Epoch 20: loss=0.7327 acc=0.8420 | val_loss=0.7472 val_acc=0.8516 | ε=2.7449
Epoch 21: loss=0.7273 acc=0.8430 | val_loss=0.7157 val_acc=0.8534 | ε=2.7907
Epoch 22: loss=0.7250 acc=0.8424 | val_loss=0.7193 val_acc=0.8541 | ε=2.8357
Epoch 23: loss=0.7260 acc=0.8425 | val_loss=0.7322 val_acc=0.8543 | ε=2.8800
Epoch 24: loss=0.7202 acc=0.8453 | val_loss=0.7324 val_acc=0.8537 | ε=2.9237
Epoch 25: loss=0.7162 acc=0.8443 | val_loss=0.7318 val_acc=0.8531 | ε=2.9668
Epoch 26: loss=0.7214 acc=0.8450 | val_loss=0.7337 val_acc=0.8544 | ε=3.0094
Epoch 27: loss=0.7220 acc=0.8443 | val_loss=0.7341 val_acc=0.8546 | ε=3.0514
Epoch 28: loss=0.7177 acc=0.8465 | val_loss=0.7424 val_acc=0.8548 | ε=3.0930
Epoch 29: loss=0.7197 acc=0.8458 | val_loss=0.7258 val_acc=0.8540 | ε=3.1340
Epoch 30: loss=0.7154 acc=0.8474 | val_loss=0.7281 val_acc=0.8545 | ε=3.1746
Epoch 31: loss=0.7139 acc=0.8475 | val_loss=0.7347 val_acc=0.8558 | ε=3.2149
Epoch 32: loss=0.7209 acc=0.8475 | val_loss=0.7417 val_acc=0.8557 | ε=3.2547
Epoch 33: loss=0.7169 acc=0.8475 | val_loss=0.7243 val_acc=0.8570 | ε=3.2941
Epoch 34: loss=0.7150 acc=0.8481 | val_loss=0.7339 val_acc=0.8570 | ε=3.3331
Epoch 35: loss=0.7196 acc=0.8491 | val_loss=0.7320 val_acc=0.8571 | ε=3.3718
Epoch 36: loss=0.7163 acc=0.8488 | val_loss=0.7373 val_acc=0.8575 | ε=3.4102
Epoch 37: loss=0.7131 acc=0.8508 | val_loss=0.7443 val_acc=0.8569 | ε=3.4482
Epoch 38: loss=0.7143 acc=0.8501 | val_loss=0.7358 val_acc=0.8567 | ε=3.4859
Epoch 39: loss=0.7181 acc=0.8502 | val_loss=0.7393 val_acc=0.8570 | ε=3.5233
Epoch 40: loss=0.7173 acc=0.8495 | val_loss=0.7381 val_acc=0.8570 | ε=3.5604
Epoch 41: loss=0.7189 acc=0.8503 | val_loss=0.7258 val_acc=0.8576 | ε=3.5973
Epoch 42: loss=0.7126 acc=0.8505 | val_loss=0.7286 val_acc=0.8574 | ε=3.6338
Epoch 43: loss=0.7105 acc=0.8516 | val_loss=0.7281 val_acc=0.8577 | ε=3.6701
Epoch 44: loss=0.7187 acc=0.8504 | val_loss=0.7284 val_acc=0.8580 | ε=3.7061
Epoch 45: loss=0.7130 acc=0.8514 | val_loss=0.7337 val_acc=0.8581 | ε=3.7418
Epoch 46: loss=0.7174 acc=0.8511 | val_loss=0.7255 val_acc=0.8587 | ε=3.7773
Epoch 47: loss=0.7101 acc=0.8520 | val_loss=0.7240 val_acc=0.8590 | ε=3.8126
Epoch 48: loss=0.7138 acc=0.8510 | val_loss=0.7318 val_acc=0.8592 | ε=3.8476
Epoch 49: loss=0.7101 acc=0.8523 | val_loss=0.7300 val_acc=0.8592 | ε=3.8824
Epoch 50: loss=0.7103 acc=0.8529 | val_loss=0.7313 val_acc=0.8593 | ε=3.9170
Epoch 51: loss=0.7131 acc=0.8527 | val_loss=0.7293 val_acc=0.8594 | ε=3.9514
Epoch 52: loss=0.7106 acc=0.8523 | val_loss=0.7282 val_acc=0.8594 | ε=3.9855
Epoch 53: loss=0.7074 acc=0.8527 | val_loss=0.7273 val_acc=0.8595 | ε=4.0195
Epoch 54: loss=0.7112 acc=0.8523 | val_loss=0.7272 val_acc=0.8596 | ε=4.0532
Epoch 55: loss=0.7167 acc=0.8514 | val_loss=0.7287 val_acc=0.8593 | ε=4.0867
Epoch 56: loss=0.7142 acc=0.8523 | val_loss=0.7281 val_acc=0.8593 | ε=4.1201
Epoch 57: loss=0.7098 acc=0.8519 | val_loss=0.7270 val_acc=0.8594 | ε=4.1532
Epoch 58: loss=0.7081 acc=0.8532 | val_loss=0.7276 val_acc=0.8593 | ε=4.1862
Epoch 59: loss=0.7001 acc=0.8540 | val_loss=0.7283 val_acc=0.8593 | ε=4.2190
Epoch 60: loss=0.7133 acc=0.8518 | val_loss=0.7282 val_acc=0.8594 | ε=4.2516
✓ 0.8717 acc | 0.8508 f1 | 494.9s

[7/21]

============================================================
dp_sleep_edf_noise1_0_run1 | sleep-edf | dp | seed=42
============================================================
✓ Using cached sleep-edf
Data: train=(313922, 24) val=(66753, 24) test=(72330, 24)
Model: 12,165 params

DP Training
  Noise multiplier: 1.0
  Max grad norm: 1.0
  Delta: 1e-05
  Epochs: 60

Epoch 01: loss=0.9309 acc=0.7511 | val_loss=0.8215 val_acc=0.8006 | ε=0.1411
Epoch 02: loss=0.8485 acc=0.7889 | val_loss=0.8248 val_acc=0.8201 | ε=0.1947
Epoch 03: loss=0.8268 acc=0.8049 | val_loss=0.7886 val_acc=0.8287 | ε=0.2368
Epoch 04: loss=0.8035 acc=0.8138 | val_loss=0.7710 val_acc=0.8345 | ε=0.2729
Epoch 05: loss=0.7978 acc=0.8178 | val_loss=0.7736 val_acc=0.8364 | ε=0.3050
Epoch 06: loss=0.7883 acc=0.8210 | val_loss=0.7695 val_acc=0.8384 | ε=0.3343
Epoch 07: loss=0.7883 acc=0.8232 | val_loss=0.7921 val_acc=0.8378 | ε=0.3615
Epoch 08: loss=0.7832 acc=0.8247 | val_loss=0.7805 val_acc=0.8400 | ε=0.3870
Epoch 09: loss=0.7770 acc=0.8249 | val_loss=0.7855 val_acc=0.8401 | ε=0.4110
Epoch 10: loss=0.7762 acc=0.8274 | val_loss=0.7665 val_acc=0.8413 | ε=0.4339
Epoch 11: loss=0.7631 acc=0.8283 | val_loss=0.7668 val_acc=0.8429 | ε=0.4557
Epoch 12: loss=0.7646 acc=0.8299 | val_loss=0.7823 val_acc=0.8428 | ε=0.4767
Epoch 13: loss=0.7616 acc=0.8310 | val_loss=0.7603 val_acc=0.8425 | ε=0.4969
Epoch 14: loss=0.7586 acc=0.8304 | val_loss=0.7448 val_acc=0.8434 | ε=0.5164
Epoch 15: loss=0.7512 acc=0.8314 | val_loss=0.7566 val_acc=0.8437 | ε=0.5352
Epoch 16: loss=0.7492 acc=0.8324 | val_loss=0.7503 val_acc=0.8447 | ε=0.5536
Epoch 17: loss=0.7468 acc=0.8308 | val_loss=0.7608 val_acc=0.8433 | ε=0.5714
Epoch 18: loss=0.7355 acc=0.8329 | val_loss=0.7490 val_acc=0.8436 | ε=0.5887
Epoch 19: loss=0.7343 acc=0.8346 | val_loss=0.7469 val_acc=0.8449 | ε=0.6056
Epoch 20: loss=0.7404 acc=0.8346 | val_loss=0.7337 val_acc=0.8464 | ε=0.6221
Epoch 21: loss=0.7331 acc=0.8360 | val_loss=0.7459 val_acc=0.8447 | ε=0.6382
Epoch 22: loss=0.7347 acc=0.8361 | val_loss=0.7513 val_acc=0.8452 | ε=0.6540
Epoch 23: loss=0.7304 acc=0.8379 | val_loss=0.7434 val_acc=0.8460 | ε=0.6695
Epoch 24: loss=0.7345 acc=0.8358 | val_loss=0.7390 val_acc=0.8469 | ε=0.6847
Epoch 25: loss=0.7277 acc=0.8372 | val_loss=0.7335 val_acc=0.8486 | ε=0.6996
Epoch 26: loss=0.7268 acc=0.8384 | val_loss=0.7200 val_acc=0.8481 | ε=0.7142
Epoch 27: loss=0.7267 acc=0.8375 | val_loss=0.7308 val_acc=0.8476 | ε=0.7286
Epoch 28: loss=0.7291 acc=0.8380 | val_loss=0.7360 val_acc=0.8484 | ε=0.7428
Epoch 29: loss=0.7284 acc=0.8384 | val_loss=0.7386 val_acc=0.8477 | ε=0.7567
Epoch 30: loss=0.7231 acc=0.8386 | val_loss=0.7390 val_acc=0.8478 | ε=0.7704
Epoch 31: loss=0.7248 acc=0.8387 | val_loss=0.7304 val_acc=0.8493 | ε=0.7839
Epoch 32: loss=0.7123 acc=0.8408 | val_loss=0.7324 val_acc=0.8486 | ε=0.7972
Epoch 33: loss=0.7213 acc=0.8386 | val_loss=0.7249 val_acc=0.8483 | ε=0.8103
Epoch 34: loss=0.7134 acc=0.8405 | val_loss=0.7278 val_acc=0.8495 | ε=0.8233
Epoch 35: loss=0.7196 acc=0.8402 | val_loss=0.7353 val_acc=0.8490 | ε=0.8361
Epoch 36: loss=0.7127 acc=0.8416 | val_loss=0.7321 val_acc=0.8494 | ε=0.8487
Epoch 37: loss=0.7111 acc=0.8417 | val_loss=0.7363 val_acc=0.8500 | ε=0.8611
Epoch 38: loss=0.7200 acc=0.8403 | val_loss=0.7275 val_acc=0.8502 | ε=0.8735
Epoch 39: loss=0.7137 acc=0.8421 | val_loss=0.7324 val_acc=0.8496 | ε=0.8856
Epoch 40: loss=0.7154 acc=0.8425 | val_loss=0.7386 val_acc=0.8491 | ε=0.8977
Epoch 41: loss=0.7118 acc=0.8420 | val_loss=0.7316 val_acc=0.8500 | ε=0.9096
Epoch 42: loss=0.7084 acc=0.8434 | val_loss=0.7399 val_acc=0.8493 | ε=0.9213
Epoch 43: loss=0.7126 acc=0.8428 | val_loss=0.7302 val_acc=0.8498 | ε=0.9330
Epoch 44: loss=0.7140 acc=0.8424 | val_loss=0.7316 val_acc=0.8498 | ε=0.9445
Epoch 45: loss=0.7137 acc=0.8429 | val_loss=0.7299 val_acc=0.8505 | ε=0.9559
Epoch 46: loss=0.7122 acc=0.8434 | val_loss=0.7352 val_acc=0.8502 | ε=0.9672
Epoch 47: loss=0.7133 acc=0.8431 | val_loss=0.7337 val_acc=0.8504 | ε=0.9784
Epoch 48: loss=0.7061 acc=0.8449 | val_loss=0.7358 val_acc=0.8503 | ε=0.9895
Epoch 49: loss=0.7085 acc=0.8446 | val_loss=0.7295 val_acc=0.8506 | ε=1.0005
Epoch 50: loss=0.7117 acc=0.8428 | val_loss=0.7312 val_acc=0.8503 | ε=1.0114
Epoch 51: loss=0.7106 acc=0.8435 | val_loss=0.7309 val_acc=0.8505 | ε=1.0221
Epoch 52: loss=0.7093 acc=0.8436 | val_loss=0.7334 val_acc=0.8506 | ε=1.0328
Epoch 53: loss=0.7081 acc=0.8446 | val_loss=0.7341 val_acc=0.8504 | ε=1.0434
Epoch 54: loss=0.7123 acc=0.8444 | val_loss=0.7353 val_acc=0.8504 | ε=1.0540
Epoch 55: loss=0.7127 acc=0.8432 | val_loss=0.7331 val_acc=0.8504 | ε=1.0644
Epoch 56: loss=0.7146 acc=0.8433 | val_loss=0.7327 val_acc=0.8505 | ε=1.0747
Epoch 57: loss=0.7142 acc=0.8439 | val_loss=0.7333 val_acc=0.8504 | ε=1.0850
Epoch 58: loss=0.7210 acc=0.8421 | val_loss=0.7328 val_acc=0.8504 | ε=1.0952
Epoch 59: loss=0.7141 acc=0.8434 | val_loss=0.7326 val_acc=0.8505 | ε=1.1053
Epoch 60: loss=0.7086 acc=0.8437 | val_loss=0.7327 val_acc=0.8504 | ε=1.1153
✓ 0.8652 acc | 0.8416 f1 | 470.7s

[8/21]

============================================================
dp_sleep_edf_noise1_0_run2 | sleep-edf | dp | seed=123
============================================================
✓ Using cached sleep-edf
Data: train=(313922, 24) val=(66753, 24) test=(72330, 24)
Model: 12,165 params

DP Training
  Noise multiplier: 1.0
  Max grad norm: 1.0
  Delta: 1e-05
  Epochs: 60

Epoch 01: loss=0.9164 acc=0.7497 | val_loss=0.8364 val_acc=0.8010 | ε=0.1411
Epoch 02: loss=0.8432 acc=0.7896 | val_loss=0.8395 val_acc=0.8193 | ε=0.1947
Epoch 03: loss=0.8205 acc=0.8056 | val_loss=0.8055 val_acc=0.8288 | ε=0.2368
Epoch 04: loss=0.8067 acc=0.8122 | val_loss=0.8065 val_acc=0.8327 | ε=0.2729
Epoch 05: loss=0.8030 acc=0.8181 | val_loss=0.7935 val_acc=0.8344 | ε=0.3050
Epoch 06: loss=0.7937 acc=0.8205 | val_loss=0.7704 val_acc=0.8382 | ε=0.3343
Epoch 07: loss=0.7857 acc=0.8220 | val_loss=0.7668 val_acc=0.8378 | ε=0.3615
Epoch 08: loss=0.7820 acc=0.8257 | val_loss=0.7675 val_acc=0.8405 | ε=0.3870
Epoch 09: loss=0.7762 acc=0.8263 | val_loss=0.7655 val_acc=0.8405 | ε=0.4110
Epoch 10: loss=0.7733 acc=0.8277 | val_loss=0.7670 val_acc=0.8414 | ε=0.4339
Epoch 11: loss=0.7720 acc=0.8272 | val_loss=0.7496 val_acc=0.8406 | ε=0.4557
Epoch 12: loss=0.7674 acc=0.8284 | val_loss=0.7670 val_acc=0.8422 | ε=0.4767
Epoch 13: loss=0.7693 acc=0.8302 | val_loss=0.7659 val_acc=0.8430 | ε=0.4969
Epoch 14: loss=0.7675 acc=0.8294 | val_loss=0.7539 val_acc=0.8431 | ε=0.5164
Epoch 15: loss=0.7620 acc=0.8299 | val_loss=0.7546 val_acc=0.8433 | ε=0.5352
Epoch 16: loss=0.7499 acc=0.8315 | val_loss=0.7486 val_acc=0.8443 | ε=0.5536
Epoch 17: loss=0.7509 acc=0.8319 | val_loss=0.7609 val_acc=0.8442 | ε=0.5714
Epoch 18: loss=0.7537 acc=0.8323 | val_loss=0.7577 val_acc=0.8432 | ε=0.5887
Epoch 19: loss=0.7468 acc=0.8330 | val_loss=0.7540 val_acc=0.8445 | ε=0.6056
Epoch 20: loss=0.7406 acc=0.8355 | val_loss=0.7562 val_acc=0.8448 | ε=0.6221
Epoch 21: loss=0.7421 acc=0.8353 | val_loss=0.7534 val_acc=0.8454 | ε=0.6382
Epoch 22: loss=0.7425 acc=0.8356 | val_loss=0.7635 val_acc=0.8460 | ε=0.6540
Epoch 23: loss=0.7401 acc=0.8362 | val_loss=0.7471 val_acc=0.8462 | ε=0.6695
Epoch 24: loss=0.7355 acc=0.8363 | val_loss=0.7595 val_acc=0.8456 | ε=0.6847
Epoch 25: loss=0.7381 acc=0.8368 | val_loss=0.7429 val_acc=0.8468 | ε=0.6996
Epoch 26: loss=0.7363 acc=0.8372 | val_loss=0.7530 val_acc=0.8471 | ε=0.7142
Epoch 27: loss=0.7344 acc=0.8368 | val_loss=0.7445 val_acc=0.8466 | ε=0.7286
Epoch 28: loss=0.7312 acc=0.8379 | val_loss=0.7434 val_acc=0.8473 | ε=0.7428
Epoch 29: loss=0.7307 acc=0.8386 | val_loss=0.7473 val_acc=0.8471 | ε=0.7567
Epoch 30: loss=0.7336 acc=0.8379 | val_loss=0.7357 val_acc=0.8475 | ε=0.7704
Epoch 31: loss=0.7318 acc=0.8388 | val_loss=0.7388 val_acc=0.8489 | ε=0.7839
Epoch 32: loss=0.7340 acc=0.8383 | val_loss=0.7310 val_acc=0.8498 | ε=0.7972
Epoch 33: loss=0.7259 acc=0.8404 | val_loss=0.7370 val_acc=0.8497 | ε=0.8103
Epoch 34: loss=0.7301 acc=0.8400 | val_loss=0.7396 val_acc=0.8486 | ε=0.8233
Epoch 35: loss=0.7289 acc=0.8399 | val_loss=0.7417 val_acc=0.8491 | ε=0.8361
Epoch 36: loss=0.7285 acc=0.8398 | val_loss=0.7368 val_acc=0.8495 | ε=0.8487
Epoch 37: loss=0.7222 acc=0.8420 | val_loss=0.7359 val_acc=0.8488 | ε=0.8611
Epoch 38: loss=0.7266 acc=0.8407 | val_loss=0.7336 val_acc=0.8491 | ε=0.8735
Epoch 39: loss=0.7220 acc=0.8410 | val_loss=0.7374 val_acc=0.8485 | ε=0.8856
Epoch 40: loss=0.7232 acc=0.8417 | val_loss=0.7348 val_acc=0.8498 | ε=0.8977
Epoch 41: loss=0.7200 acc=0.8423 | val_loss=0.7321 val_acc=0.8502 | ε=0.9096
Epoch 42: loss=0.7221 acc=0.8427 | val_loss=0.7359 val_acc=0.8495 | ε=0.9213
Epoch 43: loss=0.7220 acc=0.8431 | val_loss=0.7341 val_acc=0.8501 | ε=0.9330
Epoch 44: loss=0.7217 acc=0.8422 | val_loss=0.7282 val_acc=0.8504 | ε=0.9445
Epoch 45: loss=0.7164 acc=0.8438 | val_loss=0.7390 val_acc=0.8507 | ε=0.9559
Epoch 46: loss=0.7237 acc=0.8423 | val_loss=0.7402 val_acc=0.8507 | ε=0.9672
Epoch 47: loss=0.7208 acc=0.8429 | val_loss=0.7332 val_acc=0.8511 | ε=0.9784
Epoch 48: loss=0.7150 acc=0.8439 | val_loss=0.7362 val_acc=0.8508 | ε=0.9895
Epoch 49: loss=0.7219 acc=0.8433 | val_loss=0.7360 val_acc=0.8505 | ε=1.0005
Epoch 50: loss=0.7180 acc=0.8433 | val_loss=0.7318 val_acc=0.8507 | ε=1.0114
Epoch 51: loss=0.7137 acc=0.8444 | val_loss=0.7373 val_acc=0.8509 | ε=1.0221
Epoch 52: loss=0.7234 acc=0.8433 | val_loss=0.7358 val_acc=0.8506 | ε=1.0328
Epoch 53: loss=0.7153 acc=0.8446 | val_loss=0.7372 val_acc=0.8509 | ε=1.0434
Epoch 54: loss=0.7197 acc=0.8435 | val_loss=0.7373 val_acc=0.8511 | ε=1.0540
Epoch 55: loss=0.7137 acc=0.8448 | val_loss=0.7376 val_acc=0.8511 | ε=1.0644
Epoch 56: loss=0.7239 acc=0.8433 | val_loss=0.7364 val_acc=0.8509 | ε=1.0747
Epoch 57: loss=0.7181 acc=0.8441 | val_loss=0.7368 val_acc=0.8511 | ε=1.0850
Epoch 58: loss=0.7204 acc=0.8434 | val_loss=0.7370 val_acc=0.8511 | ε=1.0952
Epoch 59: loss=0.7193 acc=0.8437 | val_loss=0.7369 val_acc=0.8511 | ε=1.1053
Epoch 60: loss=0.7159 acc=0.8444 | val_loss=0.7369 val_acc=0.8510 | ε=1.1153
✓ 0.8660 acc | 0.8430 f1 | 547.4s

[9/21]

============================================================
dp_sleep_edf_noise1_0_run3 | sleep-edf | dp | seed=456
============================================================
✓ Using cached sleep-edf
Data: train=(313922, 24) val=(66753, 24) test=(72330, 24)
Model: 12,165 params

DP Training
  Noise multiplier: 1.0
  Max grad norm: 1.0
  Delta: 1e-05
  Epochs: 60

Epoch 01: loss=0.9497 acc=0.7478 | val_loss=0.8496 val_acc=0.7992 | ε=0.1411
Epoch 02: loss=0.8546 acc=0.7879 | val_loss=0.7991 val_acc=0.8201 | ε=0.1947
Epoch 03: loss=0.8265 acc=0.8063 | val_loss=0.7857 val_acc=0.8303 | ε=0.2368
Epoch 04: loss=0.8190 acc=0.8129 | val_loss=0.7694 val_acc=0.8337 | ε=0.2729
Epoch 05: loss=0.8103 acc=0.8174 | val_loss=0.7755 val_acc=0.8384 | ε=0.3050
Epoch 06: loss=0.7971 acc=0.8220 | val_loss=0.7756 val_acc=0.8391 | ε=0.3343
Epoch 07: loss=0.7901 acc=0.8248 | val_loss=0.7553 val_acc=0.8407 | ε=0.3615
Epoch 08: loss=0.7770 acc=0.8263 | val_loss=0.7545 val_acc=0.8421 | ε=0.3870
Epoch 09: loss=0.7794 acc=0.8279 | val_loss=0.7448 val_acc=0.8435 | ε=0.4110
Epoch 10: loss=0.7748 acc=0.8280 | val_loss=0.7628 val_acc=0.8432 | ε=0.4339
Epoch 11: loss=0.7683 acc=0.8299 | val_loss=0.7517 val_acc=0.8432 | ε=0.4557
Epoch 12: loss=0.7631 acc=0.8313 | val_loss=0.7579 val_acc=0.8437 | ε=0.4767
Epoch 13: loss=0.7584 acc=0.8317 | val_loss=0.7515 val_acc=0.8439 | ε=0.4969
Epoch 14: loss=0.7601 acc=0.8319 | val_loss=0.7311 val_acc=0.8450 | ε=0.5164
Epoch 15: loss=0.7503 acc=0.8327 | val_loss=0.7365 val_acc=0.8452 | ε=0.5352
Epoch 16: loss=0.7595 acc=0.8322 | val_loss=0.7452 val_acc=0.8455 | ε=0.5536
Epoch 17: loss=0.7507 acc=0.8338 | val_loss=0.7279 val_acc=0.8471 | ε=0.5714
Epoch 18: loss=0.7566 acc=0.8335 | val_loss=0.7387 val_acc=0.8471 | ε=0.5887
Epoch 19: loss=0.7534 acc=0.8338 | val_loss=0.7274 val_acc=0.8470 | ε=0.6056
Epoch 20: loss=0.7505 acc=0.8345 | val_loss=0.7355 val_acc=0.8470 | ε=0.6221
Epoch 21: loss=0.7425 acc=0.8352 | val_loss=0.7205 val_acc=0.8483 | ε=0.6382
Epoch 22: loss=0.7414 acc=0.8348 | val_loss=0.7179 val_acc=0.8492 | ε=0.6540
Epoch 23: loss=0.7387 acc=0.8357 | val_loss=0.7257 val_acc=0.8488 | ε=0.6695
Epoch 24: loss=0.7349 acc=0.8373 | val_loss=0.7257 val_acc=0.8480 | ε=0.6847
Epoch 25: loss=0.7293 acc=0.8371 | val_loss=0.7226 val_acc=0.8479 | ε=0.6996
Epoch 26: loss=0.7340 acc=0.8372 | val_loss=0.7219 val_acc=0.8492 | ε=0.7142
Epoch 27: loss=0.7359 acc=0.8364 | val_loss=0.7303 val_acc=0.8491 | ε=0.7286
Epoch 28: loss=0.7299 acc=0.8387 | val_loss=0.7345 val_acc=0.8480 | ε=0.7428
Epoch 29: loss=0.7345 acc=0.8379 | val_loss=0.7280 val_acc=0.8478 | ε=0.7567
Epoch 30: loss=0.7270 acc=0.8391 | val_loss=0.7276 val_acc=0.8485 | ε=0.7704
Epoch 31: loss=0.7275 acc=0.8388 | val_loss=0.7359 val_acc=0.8491 | ε=0.7839
Epoch 32: loss=0.7301 acc=0.8393 | val_loss=0.7354 val_acc=0.8497 | ε=0.7972
Epoch 33: loss=0.7259 acc=0.8396 | val_loss=0.7299 val_acc=0.8496 | ε=0.8103
Epoch 34: loss=0.7241 acc=0.8405 | val_loss=0.7356 val_acc=0.8498 | ε=0.8233
Epoch 35: loss=0.7282 acc=0.8413 | val_loss=0.7363 val_acc=0.8493 | ε=0.8361
Epoch 36: loss=0.7241 acc=0.8408 | val_loss=0.7400 val_acc=0.8502 | ε=0.8487
Epoch 37: loss=0.7220 acc=0.8427 | val_loss=0.7476 val_acc=0.8496 | ε=0.8611
Epoch 38: loss=0.7236 acc=0.8420 | val_loss=0.7410 val_acc=0.8495 | ε=0.8735
Epoch 39: loss=0.7247 acc=0.8417 | val_loss=0.7392 val_acc=0.8499 | ε=0.8856
Epoch 40: loss=0.7223 acc=0.8416 | val_loss=0.7460 val_acc=0.8497 | ε=0.8977
Epoch 41: loss=0.7301 acc=0.8411 | val_loss=0.7373 val_acc=0.8499 | ε=0.9096
Epoch 42: loss=0.7196 acc=0.8419 | val_loss=0.7331 val_acc=0.8501 | ε=0.9213
Epoch 43: loss=0.7185 acc=0.8426 | val_loss=0.7326 val_acc=0.8499 | ε=0.9330
Epoch 44: loss=0.7228 acc=0.8421 | val_loss=0.7327 val_acc=0.8506 | ε=0.9445
Epoch 45: loss=0.7187 acc=0.8435 | val_loss=0.7350 val_acc=0.8506 | ε=0.9559
Epoch 46: loss=0.7245 acc=0.8426 | val_loss=0.7352 val_acc=0.8505 | ε=0.9672
Epoch 47: loss=0.7153 acc=0.8435 | val_loss=0.7309 val_acc=0.8510 | ε=0.9784
Epoch 48: loss=0.7187 acc=0.8424 | val_loss=0.7357 val_acc=0.8512 | ε=0.9895
Epoch 49: loss=0.7159 acc=0.8435 | val_loss=0.7348 val_acc=0.8509 | ε=1.0005
Epoch 50: loss=0.7169 acc=0.8441 | val_loss=0.7368 val_acc=0.8511 | ε=1.0114
Epoch 51: loss=0.7179 acc=0.8441 | val_loss=0.7369 val_acc=0.8510 | ε=1.0221
Epoch 52: loss=0.7170 acc=0.8434 | val_loss=0.7349 val_acc=0.8511 | ε=1.0328
Epoch 53: loss=0.7140 acc=0.8443 | val_loss=0.7346 val_acc=0.8514 | ε=1.0434
Epoch 54: loss=0.7187 acc=0.8438 | val_loss=0.7348 val_acc=0.8512 | ε=1.0540
Epoch 55: loss=0.7217 acc=0.8432 | val_loss=0.7367 val_acc=0.8508 | ε=1.0644
Epoch 56: loss=0.7203 acc=0.8442 | val_loss=0.7354 val_acc=0.8512 | ε=1.0747
Epoch 57: loss=0.7178 acc=0.8435 | val_loss=0.7344 val_acc=0.8512 | ε=1.0850
Epoch 58: loss=0.7144 acc=0.8446 | val_loss=0.7344 val_acc=0.8513 | ε=1.0952
Epoch 59: loss=0.7060 acc=0.8458 | val_loss=0.7347 val_acc=0.8513 | ε=1.1053
Epoch 60: loss=0.7174 acc=0.8435 | val_loss=0.7346 val_acc=0.8513 | ε=1.1153
✓ 0.8650 acc | 0.8435 f1 | 538.0s

[10/21]

============================================================
dp_sleep_edf_noise2_0_run1 | sleep-edf | dp | seed=42
============================================================
✓ Using cached sleep-edf
Data: train=(313922, 24) val=(66753, 24) test=(72330, 24)
Model: 12,165 params

DP Training
  Noise multiplier: 2.0
  Max grad norm: 1.0
  Delta: 1e-05
  Epochs: 60

Epoch 01: loss=0.9674 acc=0.7343 | val_loss=0.8628 val_acc=0.7869 | ε=0.0546
Epoch 02: loss=0.9050 acc=0.7688 | val_loss=0.8554 val_acc=0.8002 | ε=0.0748
Epoch 03: loss=0.8781 acc=0.7807 | val_loss=0.8283 val_acc=0.8079 | ε=0.0907
Epoch 04: loss=0.8531 acc=0.7896 | val_loss=0.8268 val_acc=0.8145 | ε=0.1043
Epoch 05: loss=0.8391 acc=0.7960 | val_loss=0.8082 val_acc=0.8205 | ε=0.1164
Epoch 06: loss=0.8297 acc=0.8013 | val_loss=0.7958 val_acc=0.8232 | ε=0.1275
Epoch 07: loss=0.8279 acc=0.8040 | val_loss=0.8106 val_acc=0.8251 | ε=0.1377
Epoch 08: loss=0.8195 acc=0.8074 | val_loss=0.8001 val_acc=0.8285 | ε=0.1473
Epoch 09: loss=0.8126 acc=0.8090 | val_loss=0.8129 val_acc=0.8297 | ε=0.1563
Epoch 10: loss=0.8141 acc=0.8110 | val_loss=0.7879 val_acc=0.8304 | ε=0.1649
Epoch 11: loss=0.8027 acc=0.8125 | val_loss=0.7921 val_acc=0.8335 | ε=0.1731
Epoch 12: loss=0.8054 acc=0.8147 | val_loss=0.7928 val_acc=0.8336 | ε=0.1810
Epoch 13: loss=0.8039 acc=0.8161 | val_loss=0.7686 val_acc=0.8350 | ε=0.1885
Epoch 14: loss=0.7961 acc=0.8159 | val_loss=0.7640 val_acc=0.8335 | ε=0.1958
Epoch 15: loss=0.7901 acc=0.8175 | val_loss=0.7698 val_acc=0.8350 | ε=0.2029
Epoch 16: loss=0.7852 acc=0.8183 | val_loss=0.7652 val_acc=0.8369 | ε=0.2098
Epoch 17: loss=0.7871 acc=0.8174 | val_loss=0.7744 val_acc=0.8359 | ε=0.2164
Epoch 18: loss=0.7732 acc=0.8192 | val_loss=0.7514 val_acc=0.8371 | ε=0.2229
Epoch 19: loss=0.7653 acc=0.8211 | val_loss=0.7676 val_acc=0.8373 | ε=0.2293
Epoch 20: loss=0.7769 acc=0.8216 | val_loss=0.7498 val_acc=0.8383 | ε=0.2354
Epoch 21: loss=0.7660 acc=0.8239 | val_loss=0.7643 val_acc=0.8380 | ε=0.2415
Epoch 22: loss=0.7696 acc=0.8233 | val_loss=0.7708 val_acc=0.8377 | ε=0.2474
Epoch 23: loss=0.7645 acc=0.8248 | val_loss=0.7631 val_acc=0.8377 | ε=0.2532
Epoch 24: loss=0.7702 acc=0.8229 | val_loss=0.7578 val_acc=0.8385 | ε=0.2589
Epoch 25: loss=0.7623 acc=0.8245 | val_loss=0.7499 val_acc=0.8407 | ε=0.2644
Epoch 26: loss=0.7648 acc=0.8255 | val_loss=0.7392 val_acc=0.8398 | ε=0.2699
Epoch 27: loss=0.7605 acc=0.8256 | val_loss=0.7378 val_acc=0.8403 | ε=0.2753
Epoch 28: loss=0.7645 acc=0.8258 | val_loss=0.7478 val_acc=0.8408 | ε=0.2805
Epoch 29: loss=0.7624 acc=0.8261 | val_loss=0.7561 val_acc=0.8405 | ε=0.2857
Epoch 30: loss=0.7607 acc=0.8261 | val_loss=0.7511 val_acc=0.8404 | ε=0.2908
Epoch 31: loss=0.7594 acc=0.8267 | val_loss=0.7397 val_acc=0.8420 | ε=0.2959
Epoch 32: loss=0.7449 acc=0.8291 | val_loss=0.7448 val_acc=0.8418 | ε=0.3008
Epoch 33: loss=0.7561 acc=0.8269 | val_loss=0.7359 val_acc=0.8417 | ε=0.3057
Epoch 34: loss=0.7497 acc=0.8285 | val_loss=0.7508 val_acc=0.8415 | ε=0.3106
Epoch 35: loss=0.7501 acc=0.8278 | val_loss=0.7442 val_acc=0.8418 | ε=0.3153
Epoch 36: loss=0.7438 acc=0.8290 | val_loss=0.7458 val_acc=0.8427 | ε=0.3200
Epoch 37: loss=0.7410 acc=0.8296 | val_loss=0.7376 val_acc=0.8429 | ε=0.3247
Epoch 38: loss=0.7470 acc=0.8283 | val_loss=0.7369 val_acc=0.8431 | ε=0.3292
Epoch 39: loss=0.7415 acc=0.8302 | val_loss=0.7403 val_acc=0.8430 | ε=0.3338
Epoch 40: loss=0.7443 acc=0.8305 | val_loss=0.7436 val_acc=0.8434 | ε=0.3383
Epoch 41: loss=0.7445 acc=0.8301 | val_loss=0.7418 val_acc=0.8429 | ε=0.3427
Epoch 42: loss=0.7393 acc=0.8314 | val_loss=0.7428 val_acc=0.8432 | ε=0.3471
Epoch 43: loss=0.7427 acc=0.8306 | val_loss=0.7398 val_acc=0.8434 | ε=0.3514
Epoch 44: loss=0.7424 acc=0.8308 | val_loss=0.7380 val_acc=0.8432 | ε=0.3557
Epoch 45: loss=0.7400 acc=0.8307 | val_loss=0.7376 val_acc=0.8434 | ε=0.3599
Epoch 46: loss=0.7375 acc=0.8313 | val_loss=0.7390 val_acc=0.8437 | ε=0.3641
Epoch 47: loss=0.7375 acc=0.8313 | val_loss=0.7402 val_acc=0.8435 | ε=0.3683
Epoch 48: loss=0.7335 acc=0.8330 | val_loss=0.7441 val_acc=0.8435 | ε=0.3724
Epoch 49: loss=0.7353 acc=0.8330 | val_loss=0.7393 val_acc=0.8435 | ε=0.3765
Epoch 50: loss=0.7383 acc=0.8320 | val_loss=0.7413 val_acc=0.8435 | ε=0.3805
Epoch 51: loss=0.7386 acc=0.8321 | val_loss=0.7408 val_acc=0.8439 | ε=0.3845
Epoch 52: loss=0.7365 acc=0.8321 | val_loss=0.7417 val_acc=0.8440 | ε=0.3885
Epoch 53: loss=0.7377 acc=0.8331 | val_loss=0.7429 val_acc=0.8438 | ε=0.3924
Epoch 54: loss=0.7408 acc=0.8321 | val_loss=0.7433 val_acc=0.8439 | ε=0.3963
Epoch 55: loss=0.7404 acc=0.8316 | val_loss=0.7420 val_acc=0.8438 | ε=0.4002
Epoch 56: loss=0.7436 acc=0.8319 | val_loss=0.7422 val_acc=0.8439 | ε=0.4040
Epoch 57: loss=0.7427 acc=0.8322 | val_loss=0.7425 val_acc=0.8439 | ε=0.4078
Epoch 58: loss=0.7472 acc=0.8306 | val_loss=0.7419 val_acc=0.8438 | ε=0.4116
Epoch 59: loss=0.7432 acc=0.8318 | val_loss=0.7419 val_acc=0.8439 | ε=0.4153
Epoch 60: loss=0.7377 acc=0.8319 | val_loss=0.7418 val_acc=0.8439 | ε=0.4190
✓ 0.8583 acc | 0.8369 f1 | 780.4s

[11/21]

============================================================
dp_sleep_edf_noise2_0_run2 | sleep-edf | dp | seed=123
============================================================
✓ Using cached sleep-edf
Data: train=(313922, 24) val=(66753, 24) test=(72330, 24)
Model: 12,165 params

DP Training
  Noise multiplier: 2.0
  Max grad norm: 1.0
  Delta: 1e-05
  Epochs: 60

Epoch 01: loss=0.9501 acc=0.7334 | val_loss=0.8616 val_acc=0.7846 | ε=0.0546
Epoch 02: loss=0.8979 acc=0.7666 | val_loss=0.8617 val_acc=0.7991 | ε=0.0748
Epoch 03: loss=0.8668 acc=0.7798 | val_loss=0.8401 val_acc=0.8089 | ε=0.0907
Epoch 04: loss=0.8519 acc=0.7876 | val_loss=0.8546 val_acc=0.8125 | ε=0.1043
Epoch 05: loss=0.8416 acc=0.7951 | val_loss=0.8342 val_acc=0.8183 | ε=0.1164
Epoch 06: loss=0.8356 acc=0.8009 | val_loss=0.8089 val_acc=0.8232 | ε=0.1275
Epoch 07: loss=0.8252 acc=0.8037 | val_loss=0.7816 val_acc=0.8274 | ε=0.1377
Epoch 08: loss=0.8193 acc=0.8098 | val_loss=0.7949 val_acc=0.8288 | ε=0.1473
Epoch 09: loss=0.8179 acc=0.8108 | val_loss=0.7876 val_acc=0.8321 | ε=0.1563
Epoch 10: loss=0.8167 acc=0.8132 | val_loss=0.7991 val_acc=0.8331 | ε=0.1649
Epoch 11: loss=0.8119 acc=0.8128 | val_loss=0.7775 val_acc=0.8337 | ε=0.1731
Epoch 12: loss=0.8098 acc=0.8146 | val_loss=0.7830 val_acc=0.8345 | ε=0.1810
Epoch 13: loss=0.8081 acc=0.8162 | val_loss=0.7936 val_acc=0.8351 | ε=0.1885
Epoch 14: loss=0.8070 acc=0.8156 | val_loss=0.7748 val_acc=0.8360 | ε=0.1958
Epoch 15: loss=0.8002 acc=0.8172 | val_loss=0.7805 val_acc=0.8365 | ε=0.2029
Epoch 16: loss=0.7891 acc=0.8187 | val_loss=0.7715 val_acc=0.8369 | ε=0.2098
Epoch 17: loss=0.7871 acc=0.8190 | val_loss=0.7714 val_acc=0.8375 | ε=0.2164
Epoch 18: loss=0.7902 acc=0.8192 | val_loss=0.7869 val_acc=0.8357 | ε=0.2229
Epoch 19: loss=0.7823 acc=0.8200 | val_loss=0.7705 val_acc=0.8367 | ε=0.2293
Epoch 20: loss=0.7740 acc=0.8228 | val_loss=0.7720 val_acc=0.8386 | ε=0.2354
Epoch 21: loss=0.7776 acc=0.8231 | val_loss=0.7776 val_acc=0.8376 | ε=0.2415
Epoch 22: loss=0.7759 acc=0.8235 | val_loss=0.7944 val_acc=0.8381 | ε=0.2474
Epoch 23: loss=0.7753 acc=0.8243 | val_loss=0.7704 val_acc=0.8390 | ε=0.2532
Epoch 24: loss=0.7679 acc=0.8242 | val_loss=0.7804 val_acc=0.8383 | ε=0.2589
Epoch 25: loss=0.7698 acc=0.8246 | val_loss=0.7701 val_acc=0.8398 | ε=0.2644
Epoch 26: loss=0.7705 acc=0.8250 | val_loss=0.7782 val_acc=0.8405 | ε=0.2699
Epoch 27: loss=0.7678 acc=0.8244 | val_loss=0.7605 val_acc=0.8402 | ε=0.2753
Epoch 28: loss=0.7648 acc=0.8258 | val_loss=0.7632 val_acc=0.8402 | ε=0.2805
Epoch 29: loss=0.7605 acc=0.8266 | val_loss=0.7725 val_acc=0.8396 | ε=0.2857
Epoch 30: loss=0.7659 acc=0.8255 | val_loss=0.7595 val_acc=0.8391 | ε=0.2908
Epoch 31: loss=0.7639 acc=0.8266 | val_loss=0.7603 val_acc=0.8410 | ε=0.2959
Epoch 32: loss=0.7619 acc=0.8267 | val_loss=0.7531 val_acc=0.8420 | ε=0.3008
Epoch 33: loss=0.7545 acc=0.8290 | val_loss=0.7564 val_acc=0.8418 | ε=0.3057
Epoch 34: loss=0.7578 acc=0.8290 | val_loss=0.7630 val_acc=0.8413 | ε=0.3106
Epoch 35: loss=0.7592 acc=0.8281 | val_loss=0.7663 val_acc=0.8422 | ε=0.3153
Epoch 36: loss=0.7565 acc=0.8283 | val_loss=0.7607 val_acc=0.8422 | ε=0.3200
Epoch 37: loss=0.7500 acc=0.8302 | val_loss=0.7537 val_acc=0.8419 | ε=0.3247
Epoch 38: loss=0.7529 acc=0.8292 | val_loss=0.7542 val_acc=0.8421 | ε=0.3292
Epoch 39: loss=0.7498 acc=0.8297 | val_loss=0.7561 val_acc=0.8420 | ε=0.3338
Epoch 40: loss=0.7509 acc=0.8305 | val_loss=0.7598 val_acc=0.8423 | ε=0.3383
Epoch 41: loss=0.7484 acc=0.8306 | val_loss=0.7480 val_acc=0.8421 | ε=0.3427
Epoch 42: loss=0.7457 acc=0.8306 | val_loss=0.7503 val_acc=0.8423 | ε=0.3471
Epoch 43: loss=0.7439 acc=0.8320 | val_loss=0.7554 val_acc=0.8427 | ε=0.3514
Epoch 44: loss=0.7492 acc=0.8309 | val_loss=0.7517 val_acc=0.8427 | ε=0.3557
Epoch 45: loss=0.7419 acc=0.8325 | val_loss=0.7569 val_acc=0.8428 | ε=0.3599
Epoch 46: loss=0.7494 acc=0.8312 | val_loss=0.7584 val_acc=0.8429 | ε=0.3641
Epoch 47: loss=0.7480 acc=0.8317 | val_loss=0.7525 val_acc=0.8428 | ε=0.3683
Epoch 48: loss=0.7383 acc=0.8323 | val_loss=0.7525 val_acc=0.8428 | ε=0.3724
Epoch 49: loss=0.7484 acc=0.8315 | val_loss=0.7550 val_acc=0.8426 | ε=0.3765
Epoch 50: loss=0.7431 acc=0.8322 | val_loss=0.7515 val_acc=0.8427 | ε=0.3805
Epoch 51: loss=0.7374 acc=0.8330 | val_loss=0.7541 val_acc=0.8426 | ε=0.3845
Epoch 52: loss=0.7469 acc=0.8318 | val_loss=0.7541 val_acc=0.8426 | ε=0.3885
Epoch 53: loss=0.7404 acc=0.8334 | val_loss=0.7556 val_acc=0.8427 | ε=0.3924
Epoch 54: loss=0.7443 acc=0.8324 | val_loss=0.7560 val_acc=0.8427 | ε=0.3963
Epoch 55: loss=0.7374 acc=0.8340 | val_loss=0.7565 val_acc=0.8429 | ε=0.4002
Epoch 56: loss=0.7480 acc=0.8320 | val_loss=0.7566 val_acc=0.8427 | ε=0.4040
Epoch 57: loss=0.7433 acc=0.8328 | val_loss=0.7565 val_acc=0.8427 | ε=0.4078
Epoch 58: loss=0.7460 acc=0.8327 | val_loss=0.7565 val_acc=0.8427 | ε=0.4116
Epoch 59: loss=0.7438 acc=0.8325 | val_loss=0.7564 val_acc=0.8427 | ε=0.4153
Epoch 60: loss=0.7416 acc=0.8334 | val_loss=0.7564 val_acc=0.8427 | ε=0.4190
✓ 0.8584 acc | 0.8359 f1 | 470.0s

[12/21]

============================================================
dp_sleep_edf_noise2_0_run3 | sleep-edf | dp | seed=456
============================================================
✓ Using cached sleep-edf
Data: train=(313922, 24) val=(66753, 24) test=(72330, 24)
Model: 12,165 params

DP Training
  Noise multiplier: 2.0
  Max grad norm: 1.0
  Delta: 1e-05
  Epochs: 60

Epoch 01: loss=0.9887 acc=0.7297 | val_loss=0.8820 val_acc=0.7866 | ε=0.0546
Epoch 02: loss=0.9120 acc=0.7669 | val_loss=0.8360 val_acc=0.7976 | ε=0.0748
Epoch 03: loss=0.8737 acc=0.7789 | val_loss=0.8141 val_acc=0.8085 | ε=0.0907
Epoch 04: loss=0.8635 acc=0.7871 | val_loss=0.7923 val_acc=0.8177 | ε=0.1043
Epoch 05: loss=0.8536 acc=0.7944 | val_loss=0.7951 val_acc=0.8233 | ε=0.1164
Epoch 06: loss=0.8392 acc=0.8018 | val_loss=0.7937 val_acc=0.8277 | ε=0.1275
Epoch 07: loss=0.8316 acc=0.8067 | val_loss=0.7704 val_acc=0.8317 | ε=0.1377
Epoch 08: loss=0.8176 acc=0.8098 | val_loss=0.7696 val_acc=0.8326 | ε=0.1473
Epoch 09: loss=0.8190 acc=0.8119 | val_loss=0.7689 val_acc=0.8355 | ε=0.1563
Epoch 10: loss=0.8172 acc=0.8129 | val_loss=0.7863 val_acc=0.8356 | ε=0.1649
Epoch 11: loss=0.8106 acc=0.8157 | val_loss=0.7588 val_acc=0.8375 | ε=0.1731
Epoch 12: loss=0.7993 acc=0.8181 | val_loss=0.7728 val_acc=0.8378 | ε=0.1810
Epoch 13: loss=0.8004 acc=0.8187 | val_loss=0.7642 val_acc=0.8384 | ε=0.1885
Epoch 14: loss=0.8018 acc=0.8187 | val_loss=0.7500 val_acc=0.8386 | ε=0.1958
Epoch 15: loss=0.7915 acc=0.8191 | val_loss=0.7535 val_acc=0.8389 | ε=0.2029
Epoch 16: loss=0.7959 acc=0.8200 | val_loss=0.7647 val_acc=0.8391 | ε=0.2098
Epoch 17: loss=0.7884 acc=0.8216 | val_loss=0.7447 val_acc=0.8404 | ε=0.2164
Epoch 18: loss=0.7900 acc=0.8213 | val_loss=0.7543 val_acc=0.8407 | ε=0.2229
Epoch 19: loss=0.7913 acc=0.8214 | val_loss=0.7492 val_acc=0.8404 | ε=0.2293
Epoch 20: loss=0.7868 acc=0.8225 | val_loss=0.7449 val_acc=0.8403 | ε=0.2354
Epoch 21: loss=0.7791 acc=0.8233 | val_loss=0.7362 val_acc=0.8418 | ε=0.2415
Epoch 22: loss=0.7744 acc=0.8231 | val_loss=0.7286 val_acc=0.8419 | ε=0.2474
Epoch 23: loss=0.7702 acc=0.8233 | val_loss=0.7447 val_acc=0.8420 | ε=0.2532
Epoch 24: loss=0.7702 acc=0.8255 | val_loss=0.7392 val_acc=0.8413 | ε=0.2589
Epoch 25: loss=0.7632 acc=0.8251 | val_loss=0.7404 val_acc=0.8408 | ε=0.2644
Epoch 26: loss=0.7684 acc=0.8252 | val_loss=0.7416 val_acc=0.8412 | ε=0.2699
Epoch 27: loss=0.7686 acc=0.8251 | val_loss=0.7443 val_acc=0.8420 | ε=0.2753
Epoch 28: loss=0.7613 acc=0.8275 | val_loss=0.7513 val_acc=0.8414 | ε=0.2805
Epoch 29: loss=0.7660 acc=0.8266 | val_loss=0.7412 val_acc=0.8412 | ε=0.2857
Epoch 30: loss=0.7554 acc=0.8284 | val_loss=0.7409 val_acc=0.8420 | ε=0.2908
Epoch 31: loss=0.7576 acc=0.8277 | val_loss=0.7487 val_acc=0.8434 | ε=0.2959
Epoch 32: loss=0.7600 acc=0.8279 | val_loss=0.7485 val_acc=0.8426 | ε=0.3008
Epoch 33: loss=0.7578 acc=0.8283 | val_loss=0.7443 val_acc=0.8428 | ε=0.3057
Epoch 34: loss=0.7558 acc=0.8294 | val_loss=0.7420 val_acc=0.8436 | ε=0.3106
Epoch 35: loss=0.7540 acc=0.8306 | val_loss=0.7458 val_acc=0.8431 | ε=0.3153
Epoch 36: loss=0.7560 acc=0.8300 | val_loss=0.7505 val_acc=0.8433 | ε=0.3200
Epoch 37: loss=0.7531 acc=0.8317 | val_loss=0.7573 val_acc=0.8436 | ε=0.3247
Epoch 38: loss=0.7549 acc=0.8310 | val_loss=0.7517 val_acc=0.8434 | ε=0.3292
Epoch 39: loss=0.7552 acc=0.8308 | val_loss=0.7558 val_acc=0.8431 | ε=0.3338
Epoch 40: loss=0.7544 acc=0.8310 | val_loss=0.7597 val_acc=0.8429 | ε=0.3383
Epoch 41: loss=0.7599 acc=0.8310 | val_loss=0.7570 val_acc=0.8431 | ε=0.3427
Epoch 42: loss=0.7504 acc=0.8310 | val_loss=0.7510 val_acc=0.8424 | ε=0.3471
Epoch 43: loss=0.7517 acc=0.8317 | val_loss=0.7530 val_acc=0.8428 | ε=0.3514
Epoch 44: loss=0.7540 acc=0.8318 | val_loss=0.7516 val_acc=0.8427 | ε=0.3557
Epoch 45: loss=0.7477 acc=0.8326 | val_loss=0.7528 val_acc=0.8431 | ε=0.3599
Epoch 46: loss=0.7538 acc=0.8320 | val_loss=0.7545 val_acc=0.8428 | ε=0.3641
Epoch 47: loss=0.7441 acc=0.8332 | val_loss=0.7513 val_acc=0.8435 | ε=0.3683
Epoch 48: loss=0.7474 acc=0.8318 | val_loss=0.7530 val_acc=0.8437 | ε=0.3724
Epoch 49: loss=0.7444 acc=0.8326 | val_loss=0.7500 val_acc=0.8434 | ε=0.3765
Epoch 50: loss=0.7422 acc=0.8336 | val_loss=0.7539 val_acc=0.8434 | ε=0.3805
Epoch 51: loss=0.7458 acc=0.8332 | val_loss=0.7558 val_acc=0.8435 | ε=0.3845
Epoch 52: loss=0.7458 acc=0.8325 | val_loss=0.7546 val_acc=0.8437 | ε=0.3885
Epoch 53: loss=0.7419 acc=0.8340 | val_loss=0.7541 val_acc=0.8436 | ε=0.3924
Epoch 54: loss=0.7487 acc=0.8325 | val_loss=0.7537 val_acc=0.8434 | ε=0.3963
Epoch 55: loss=0.7488 acc=0.8327 | val_loss=0.7552 val_acc=0.8436 | ε=0.4002
Epoch 56: loss=0.7478 acc=0.8333 | val_loss=0.7543 val_acc=0.8437 | ε=0.4040
Epoch 57: loss=0.7451 acc=0.8332 | val_loss=0.7534 val_acc=0.8437 | ε=0.4078
Epoch 58: loss=0.7442 acc=0.8337 | val_loss=0.7533 val_acc=0.8437 | ε=0.4116
Epoch 59: loss=0.7354 acc=0.8346 | val_loss=0.7535 val_acc=0.8437 | ε=0.4153
Epoch 60: loss=0.7430 acc=0.8330 | val_loss=0.7535 val_acc=0.8437 | ε=0.4190
✓ 0.8577 acc | 0.8358 f1 | 464.7s

[13/21]

============================================================
fl_sleep_edf_c3_run1 | sleep-edf | fl | seed=42
============================================================
✓ Using cached sleep-edf
Data: train=(313922, 24) val=(66753, 24) test=(72330, 24)
Model: 12,165 params

======================================================================
FEDERATED LEARNING TRAINING
======================================================================
  Clients: 3
  Global rounds: 40
  Local epochs per round: 1
  Validation frequency: every 5 rounds
  Early stopping patience: 15 validation rounds
======================================================================

Round   2: loss=0.4751 acc=0.8350
Round   4: loss=0.4362 acc=0.8485
Round   5: loss=0.4266 acc=0.8516 | val_loss=0.4182 val_acc=0.8582
Round   6: loss=0.4209 acc=0.8537
Round   8: loss=0.4103 acc=0.8569
Round  10: loss=0.4044 acc=0.8587 | val_loss=0.4031 val_acc=0.8655
Round  12: loss=0.4003 acc=0.8600
Round  14: loss=0.3952 acc=0.8615
Round  15: loss=0.3940 acc=0.8622 | val_loss=0.3986 val_acc=0.8679
Round  16: loss=0.3941 acc=0.8616
Round  18: loss=0.3899 acc=0.8633
Round  20: loss=0.3874 acc=0.8635 | val_loss=0.4001 val_acc=0.8693
Round  22: loss=0.3852 acc=0.8650
Round  24: loss=0.3837 acc=0.8644
Round  25: loss=0.3826 acc=0.8656 | val_loss=0.3962 val_acc=0.8695
Round  26: loss=0.3822 acc=0.8658
Round  28: loss=0.3807 acc=0.8660
Round  30: loss=0.3786 acc=0.8666 | val_loss=0.3931 val_acc=0.8690
Round  32: loss=0.3767 acc=0.8677
Round  34: loss=0.3755 acc=0.8684
Round  35: loss=0.3747 acc=0.8682 | val_loss=0.3914 val_acc=0.8702
Round  36: loss=0.3739 acc=0.8687
Round  38: loss=0.3732 acc=0.8688
Round  40: loss=0.3725 acc=0.8694 | val_loss=0.3884 val_acc=0.8722

======================================================================
Training completed: 40 rounds in 73.8s
  Best validation accuracy: 0.8722 (round 40)
  Validation checks: 8
======================================================================

✓ 0.8826 acc | 0.8691 f1 | 74.0s

[14/21]

============================================================
fl_sleep_edf_c3_run2 | sleep-edf | fl | seed=123
============================================================
✓ Using cached sleep-edf
Data: train=(313922, 24) val=(66753, 24) test=(72330, 24)
Model: 12,165 params

======================================================================
FEDERATED LEARNING TRAINING
======================================================================
  Clients: 3
  Global rounds: 40
  Local epochs per round: 1
  Validation frequency: every 5 rounds
  Early stopping patience: 15 validation rounds
======================================================================

Round   2: loss=0.4776 acc=0.8335
Round   4: loss=0.4386 acc=0.8480
Round   5: loss=0.4286 acc=0.8505 | val_loss=0.4206 val_acc=0.8576
Round   6: loss=0.4223 acc=0.8532
Round   8: loss=0.4122 acc=0.8569
Round  10: loss=0.4044 acc=0.8587 | val_loss=0.4098 val_acc=0.8644
Round  12: loss=0.4004 acc=0.8603
Round  14: loss=0.3963 acc=0.8611
Round  15: loss=0.3939 acc=0.8622 | val_loss=0.4018 val_acc=0.8683
Round  16: loss=0.3915 acc=0.8630
Round  18: loss=0.3899 acc=0.8638
Round  20: loss=0.3862 acc=0.8646 | val_loss=0.3970 val_acc=0.8684
Round  22: loss=0.3851 acc=0.8646
Round  24: loss=0.3830 acc=0.8655
Round  25: loss=0.3815 acc=0.8659 | val_loss=0.3963 val_acc=0.8704
Round  26: loss=0.3814 acc=0.8662
Round  28: loss=0.3794 acc=0.8668
Round  30: loss=0.3787 acc=0.8669 | val_loss=0.3922 val_acc=0.8704
Round  32: loss=0.3766 acc=0.8676
Round  34: loss=0.3745 acc=0.8684
Round  35: loss=0.3743 acc=0.8685 | val_loss=0.3903 val_acc=0.8717
Round  36: loss=0.3747 acc=0.8686
Round  38: loss=0.3732 acc=0.8692
Round  40: loss=0.3721 acc=0.8698 | val_loss=0.3900 val_acc=0.8713

======================================================================
Training completed: 40 rounds in 63.0s
  Best validation accuracy: 0.8717 (round 35)
  Validation checks: 8
======================================================================

✓ 0.8838 acc | 0.8697 f1 | 63.3s

[15/21]

============================================================
fl_sleep_edf_c3_run3 | sleep-edf | fl | seed=456
============================================================
✓ Using cached sleep-edf
Data: train=(313922, 24) val=(66753, 24) test=(72330, 24)
Model: 12,165 params

======================================================================
FEDERATED LEARNING TRAINING
======================================================================
  Clients: 3
  Global rounds: 40
  Local epochs per round: 1
  Validation frequency: every 5 rounds
  Early stopping patience: 15 validation rounds
======================================================================

Round   2: loss=0.4822 acc=0.8327
Round   4: loss=0.4387 acc=0.8479
Round   5: loss=0.4296 acc=0.8509 | val_loss=0.4165 val_acc=0.8593
Round   6: loss=0.4226 acc=0.8531
Round   8: loss=0.4133 acc=0.8565
Round  10: loss=0.4064 acc=0.8589 | val_loss=0.4013 val_acc=0.8652
Round  12: loss=0.3990 acc=0.8608
Round  14: loss=0.3954 acc=0.8623
Round  15: loss=0.3952 acc=0.8620 | val_loss=0.3959 val_acc=0.8682
Round  16: loss=0.3926 acc=0.8631
Round  18: loss=0.3893 acc=0.8640
Round  20: loss=0.3880 acc=0.8647 | val_loss=0.3940 val_acc=0.8693
Round  22: loss=0.3865 acc=0.8650
Round  24: loss=0.3838 acc=0.8657
Round  25: loss=0.3830 acc=0.8660 | val_loss=0.3913 val_acc=0.8698
Round  26: loss=0.3820 acc=0.8660
Round  28: loss=0.3797 acc=0.8665
Round  30: loss=0.3785 acc=0.8673 | val_loss=0.3907 val_acc=0.8707
Round  32: loss=0.3781 acc=0.8674
Round  34: loss=0.3745 acc=0.8686
Round  35: loss=0.3748 acc=0.8685 | val_loss=0.3886 val_acc=0.8722
Round  36: loss=0.3748 acc=0.8686
Round  38: loss=0.3733 acc=0.8695
Round  40: loss=0.3726 acc=0.8698 | val_loss=0.3850 val_acc=0.8736

======================================================================
Training completed: 40 rounds in 73.9s
  Best validation accuracy: 0.8736 (round 40)
  Validation checks: 8
======================================================================

✓ 0.8828 acc | 0.8684 f1 | 74.2s

[16/21]

============================================================
fl_sleep_edf_c5_run1 | sleep-edf | fl | seed=42
============================================================
✓ Using cached sleep-edf
Data: train=(313922, 24) val=(66753, 24) test=(72330, 24)
Model: 12,165 params

======================================================================
FEDERATED LEARNING TRAINING
======================================================================
  Clients: 5
  Global rounds: 40
  Local epochs per round: 1
  Validation frequency: every 5 rounds
  Early stopping patience: 15 validation rounds
======================================================================

Round   2: loss=0.4978 acc=0.8252
Round   4: loss=0.4485 acc=0.8438
Round   5: loss=0.4381 acc=0.8474 | val_loss=0.4315 val_acc=0.8534
Round   6: loss=0.4312 acc=0.8502
Round   8: loss=0.4189 acc=0.8540
Round  10: loss=0.4121 acc=0.8562 | val_loss=0.4144 val_acc=0.8598
Round  12: loss=0.4061 acc=0.8575
Round  14: loss=0.4015 acc=0.8595
Round  15: loss=0.3998 acc=0.8598 | val_loss=0.4088 val_acc=0.8644
Round  16: loss=0.3975 acc=0.8610
Round  18: loss=0.3948 acc=0.8622
Round  20: loss=0.3913 acc=0.8628 | val_loss=0.4082 val_acc=0.8648
Round  22: loss=0.3887 acc=0.8637
Round  24: loss=0.3876 acc=0.8642
Round  25: loss=0.3864 acc=0.8649 | val_loss=0.4045 val_acc=0.8676
Round  26: loss=0.3855 acc=0.8646
Round  28: loss=0.3830 acc=0.8655
Round  30: loss=0.3825 acc=0.8657 | val_loss=0.4023 val_acc=0.8676
Round  32: loss=0.3813 acc=0.8659
Round  34: loss=0.3794 acc=0.8668
Round  35: loss=0.3792 acc=0.8671 | val_loss=0.3988 val_acc=0.8696
Round  36: loss=0.3793 acc=0.8671
Round  38: loss=0.3787 acc=0.8674
Round  40: loss=0.3768 acc=0.8679 | val_loss=0.3972 val_acc=0.8698

======================================================================
Training completed: 40 rounds in 82.8s
  Best validation accuracy: 0.8698 (round 40)
  Validation checks: 8
======================================================================

✓ 0.8814 acc | 0.8685 f1 | 83.0s

[17/21]

============================================================
fl_sleep_edf_c5_run2 | sleep-edf | fl | seed=123
============================================================
✓ Using cached sleep-edf
Data: train=(313922, 24) val=(66753, 24) test=(72330, 24)
Model: 12,165 params

======================================================================
FEDERATED LEARNING TRAINING
======================================================================
  Clients: 5
  Global rounds: 40
  Local epochs per round: 1
  Validation frequency: every 5 rounds
  Early stopping patience: 15 validation rounds
======================================================================

Round   2: loss=0.4983 acc=0.8251
Round   4: loss=0.4516 acc=0.8423
Round   5: loss=0.4397 acc=0.8471 | val_loss=0.4356 val_acc=0.8521
Round   6: loss=0.4307 acc=0.8499
Round   8: loss=0.4197 acc=0.8538
Round  10: loss=0.4123 acc=0.8560 | val_loss=0.4167 val_acc=0.8593
Round  12: loss=0.4068 acc=0.8577
Round  14: loss=0.4011 acc=0.8593
Round  15: loss=0.3992 acc=0.8601 | val_loss=0.4124 val_acc=0.8634
Round  16: loss=0.3973 acc=0.8610
Round  18: loss=0.3934 acc=0.8622
Round  20: loss=0.3912 acc=0.8634 | val_loss=0.4063 val_acc=0.8661
Round  22: loss=0.3888 acc=0.8639
Round  24: loss=0.3878 acc=0.8644
Round  25: loss=0.3862 acc=0.8648 | val_loss=0.4000 val_acc=0.8677
Round  26: loss=0.3854 acc=0.8655
Round  28: loss=0.3833 acc=0.8656
Round  30: loss=0.3829 acc=0.8664 | val_loss=0.4008 val_acc=0.8673
Round  32: loss=0.3798 acc=0.8674
Round  34: loss=0.3791 acc=0.8670
Round  35: loss=0.3794 acc=0.8672 | val_loss=0.3983 val_acc=0.8690
Round  36: loss=0.3789 acc=0.8676
Round  38: loss=0.3781 acc=0.8675
Round  40: loss=0.3772 acc=0.8681 | val_loss=0.3967 val_acc=0.8703

======================================================================
Training completed: 40 rounds in 65.9s
  Best validation accuracy: 0.8703 (round 40)
  Validation checks: 8
======================================================================

✓ 0.8821 acc | 0.8688 f1 | 66.1s

[18/21]

============================================================
fl_sleep_edf_c5_run3 | sleep-edf | fl | seed=456
============================================================
✓ Using cached sleep-edf
Data: train=(313922, 24) val=(66753, 24) test=(72330, 24)
Model: 12,165 params

======================================================================
FEDERATED LEARNING TRAINING
======================================================================
  Clients: 5
  Global rounds: 40
  Local epochs per round: 1
  Validation frequency: every 5 rounds
  Early stopping patience: 15 validation rounds
======================================================================

Round   2: loss=0.5036 acc=0.8227
Round   4: loss=0.4534 acc=0.8422
Round   5: loss=0.4421 acc=0.8463 | val_loss=0.4324 val_acc=0.8539
Round   6: loss=0.4355 acc=0.8491
Round   8: loss=0.4214 acc=0.8533
Round  10: loss=0.4134 acc=0.8563 | val_loss=0.4095 val_acc=0.8620
Round  12: loss=0.4065 acc=0.8580
Round  14: loss=0.4020 acc=0.8598
Round  15: loss=0.4001 acc=0.8607 | val_loss=0.4024 val_acc=0.8649
Round  16: loss=0.3986 acc=0.8604
Round  18: loss=0.3948 acc=0.8620
Round  20: loss=0.3932 acc=0.8630 | val_loss=0.3998 val_acc=0.8678
Round  22: loss=0.3898 acc=0.8639
Round  24: loss=0.3879 acc=0.8646
Round  25: loss=0.3864 acc=0.8654 | val_loss=0.3991 val_acc=0.8680
Round  26: loss=0.3855 acc=0.8648
Round  28: loss=0.3847 acc=0.8654
Round  30: loss=0.3823 acc=0.8663 | val_loss=0.3986 val_acc=0.8680
Round  32: loss=0.3804 acc=0.8667
Round  34: loss=0.3796 acc=0.8669
Round  35: loss=0.3803 acc=0.8677 | val_loss=0.3959 val_acc=0.8691
Round  36: loss=0.3781 acc=0.8677
Round  38: loss=0.3783 acc=0.8681
Round  40: loss=0.3770 acc=0.8686 | val_loss=0.3939 val_acc=0.8697

======================================================================
Training completed: 40 rounds in 69.0s
  Best validation accuracy: 0.8697 (round 40)
  Validation checks: 8
======================================================================

✓ 0.8808 acc | 0.8670 f1 | 69.3s

[19/21]

============================================================
fl_sleep_edf_c10_run1 | sleep-edf | fl | seed=42
============================================================
✓ Using cached sleep-edf
Data: train=(313922, 24) val=(66753, 24) test=(72330, 24)
Model: 12,165 params

======================================================================
FEDERATED LEARNING TRAINING
======================================================================
  Clients: 10
  Global rounds: 40
  Local epochs per round: 1
  Validation frequency: every 5 rounds
  Early stopping patience: 15 validation rounds
======================================================================

Round   2: loss=0.5376 acc=0.8082
Round   4: loss=0.4689 acc=0.8351
Round   5: loss=0.4553 acc=0.8405 | val_loss=0.4564 val_acc=0.8448
Round   6: loss=0.4444 acc=0.8444
Round   8: loss=0.4326 acc=0.8489
Round  10: loss=0.4225 acc=0.8523 | val_loss=0.4373 val_acc=0.8520
Round  12: loss=0.4152 acc=0.8546
Round  14: loss=0.4098 acc=0.8562
Round  15: loss=0.4081 acc=0.8571 | val_loss=0.4286 val_acc=0.8558
Round  16: loss=0.4048 acc=0.8578
Round  18: loss=0.4024 acc=0.8588
Round  20: loss=0.3987 acc=0.8598 | val_loss=0.4242 val_acc=0.8574
Round  22: loss=0.3962 acc=0.8606
Round  24: loss=0.3948 acc=0.8614
Round  25: loss=0.3928 acc=0.8620 | val_loss=0.4212 val_acc=0.8590
Round  26: loss=0.3917 acc=0.8626
Round  28: loss=0.3902 acc=0.8630
Round  30: loss=0.3890 acc=0.8634 | val_loss=0.4184 val_acc=0.8605
Round  32: loss=0.3880 acc=0.8638
Round  34: loss=0.3873 acc=0.8644
Round  35: loss=0.3870 acc=0.8641 | val_loss=0.4172 val_acc=0.8622
Round  36: loss=0.3870 acc=0.8643
Round  38: loss=0.3861 acc=0.8648
Round  40: loss=0.3865 acc=0.8649 | val_loss=0.4158 val_acc=0.8626

======================================================================
Training completed: 40 rounds in 76.8s
  Best validation accuracy: 0.8626 (round 40)
  Validation checks: 8
======================================================================

✓ 0.8782 acc | 0.8657 f1 | 77.0s

[20/21]

============================================================
fl_sleep_edf_c10_run2 | sleep-edf | fl | seed=123
============================================================
✓ Using cached sleep-edf
Data: train=(313922, 24) val=(66753, 24) test=(72330, 24)
Model: 12,165 params

======================================================================
FEDERATED LEARNING TRAINING
======================================================================
  Clients: 10
  Global rounds: 40
  Local epochs per round: 1
  Validation frequency: every 5 rounds
  Early stopping patience: 15 validation rounds
======================================================================

Round   2: loss=0.5368 acc=0.8086
Round   4: loss=0.4692 acc=0.8349
Round   5: loss=0.4570 acc=0.8401 | val_loss=0.4624 val_acc=0.8409
Round   6: loss=0.4458 acc=0.8433
Round   8: loss=0.4326 acc=0.8486
Round  10: loss=0.4258 acc=0.8510 | val_loss=0.4446 val_acc=0.8491
Round  12: loss=0.4190 acc=0.8537
Round  14: loss=0.4129 acc=0.8553
Round  15: loss=0.4097 acc=0.8564 | val_loss=0.4343 val_acc=0.8521
Round  16: loss=0.4077 acc=0.8574
Round  18: loss=0.4036 acc=0.8580
Round  20: loss=0.4011 acc=0.8594 | val_loss=0.4269 val_acc=0.8561
Round  22: loss=0.3979 acc=0.8602
Round  24: loss=0.3941 acc=0.8618
Round  25: loss=0.3941 acc=0.8613 | val_loss=0.4229 val_acc=0.8583
Round  26: loss=0.3936 acc=0.8616
Round  28: loss=0.3928 acc=0.8621
Round  30: loss=0.3888 acc=0.8636 | val_loss=0.4197 val_acc=0.8595
Round  32: loss=0.3884 acc=0.8640
Round  34: loss=0.3880 acc=0.8642
Round  35: loss=0.3878 acc=0.8646 | val_loss=0.4164 val_acc=0.8617
Round  36: loss=0.3877 acc=0.8643
Round  38: loss=0.3870 acc=0.8651
Round  40: loss=0.3862 acc=0.8654 | val_loss=0.4136 val_acc=0.8624

======================================================================
Training completed: 40 rounds in 126.5s
  Best validation accuracy: 0.8624 (round 40)
  Validation checks: 8
======================================================================

✓ 0.8781 acc | 0.8668 f1 | 127.0s

[21/21]

============================================================
fl_sleep_edf_c10_run3 | sleep-edf | fl | seed=456
============================================================
✓ Using cached sleep-edf
Data: train=(313922, 24) val=(66753, 24) test=(72330, 24)
Model: 12,165 params

======================================================================
FEDERATED LEARNING TRAINING
======================================================================
  Clients: 10
  Global rounds: 40
  Local epochs per round: 1
  Validation frequency: every 5 rounds
  Early stopping patience: 15 validation rounds
======================================================================

Round   2: loss=0.5426 acc=0.8066
Round   4: loss=0.4738 acc=0.8331
Round   5: loss=0.4599 acc=0.8387 | val_loss=0.4561 val_acc=0.8454
Round   6: loss=0.4498 acc=0.8430
Round   8: loss=0.4360 acc=0.8477
Round  10: loss=0.4261 acc=0.8514 | val_loss=0.4378 val_acc=0.8504
Round  12: loss=0.4196 acc=0.8537
Round  14: loss=0.4152 acc=0.8552
Round  15: loss=0.4115 acc=0.8560 | val_loss=0.4279 val_acc=0.8561
Round  16: loss=0.4091 acc=0.8571
Round  18: loss=0.4051 acc=0.8578
Round  20: loss=0.4019 acc=0.8587 | val_loss=0.4195 val_acc=0.8588
Round  22: loss=0.3990 acc=0.8603
Round  24: loss=0.3956 acc=0.8612
Round  25: loss=0.3943 acc=0.8614 | val_loss=0.4142 val_acc=0.8609
Round  26: loss=0.3941 acc=0.8619
Round  28: loss=0.3920 acc=0.8622
Round  30: loss=0.3910 acc=0.8628 | val_loss=0.4134 val_acc=0.8615
Round  32: loss=0.3900 acc=0.8633
Round  34: loss=0.3881 acc=0.8639
Round  35: loss=0.3879 acc=0.8642 | val_loss=0.4104 val_acc=0.8632
Round  36: loss=0.3884 acc=0.8645
Round  38: loss=0.3883 acc=0.8647
Round  40: loss=0.3863 acc=0.8655 | val_loss=0.4083 val_acc=0.8640

======================================================================
Training completed: 40 rounds in 81.3s
  Best validation accuracy: 0.8640 (round 40)
  Validation checks: 8
======================================================================

✓ 0.8775 acc | 0.8639 f1 | 81.6s
✓ Cache cleared

============================================================
RESULTS
============================================================
Success: 21/21
Avg Accuracy: 0.8749
Avg F1: 0.8577
Time: 1.56h
Saved: experiments/results_log.json

Total time: 93.8 minutes

((venv) ) vasco@MacBook-Air-de-Vasco mhealth-data-privacy % 