# üéØ Recomenda√ß√µes de Baseline para Paper Cient√≠fico

## üìä An√°lise dos Datasets

### Sleep-EDF
- **Samples**: 453,005 √©pocas (30s cada)
- **Formato**: Feature vectors (24 features)
- **Classes**: 5 (W, N1, N2, N3, R) - **DESBALANCEADO** (W=64%)
- **Tipo**: S√©rie temporal ‚Üí Features extra√≠das
- **Split**: Random (n√£o subject-wise)

### WESAD
- **Samples**: 1,189 janelas (60s cada)
- **Formato**: Temporal windows (14 channels √ó 1920 timesteps)
- **Classes**: 2 (non-stress, stress) - **DESBALANCEADO** (non-stress=70%)
- **Tipo**: S√©rie temporal raw
- **Split**: Subject-wise (LOSO-style) - **IDEAL para FL**

---

## üö® PROBLEMA CR√çTICO IDENTIFICADO

### ‚ö†Ô∏è Incompatibilidade de Formatos

**Sleep-EDF**: Features extra√≠das (24 dimens√µes)
**WESAD**: Temporal windows (14√ó1920 dimens√µes)

**Problema**: Formatos diferentes impossibilitam compara√ß√£o justa entre baselines!

---

## ‚úÖ SOLU√á√ÉO RECOMENDADA

### üéØ Op√ß√£o 1: Uniformizar para Temporal Windows (RECOMENDADO)

#### **Vantagens:**
- ‚úÖ Compat√≠vel com FL (necessita temporal structure)
- ‚úÖ Compat√≠vel com DP (adiciona ru√≠do aos dados raw)
- ‚úÖ Preserva informa√ß√£o temporal completa
- ‚úÖ Permite usar mesma arquitetura (LSTM) para ambos
- ‚úÖ Mais robusto e generaliz√°vel

#### **Implementa√ß√£o:**

**Sleep-EDF:**
```python
# Usar dados temporais RAW em vez de features
# Criar janelas de 30s √ó 100 Hz = 3000 samples
# Shape: (n_windows, n_channels, 3000)
# Channels: 4 (EEG Fpz-Cz, EEG Pz-Oz, EOG H, EOG V)
```

**WESAD:**
```python
# Manter formato atual
# Shape: (n_windows, 14, 1920)
# 14 channels de sinais fisiol√≥gicos
```

**Modelo Baseline:**
```python
# LSTM Bidirectional simples (compat√≠vel com ambos)
class SimpleBidirectionalLSTM(nn.Module):
    def __init__(self, input_channels, hidden_size, num_classes):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_channels, 
            hidden_size=hidden_size,
            num_layers=2,
            batch_first=True,
            bidirectional=True
        )
        self.fc = nn.Linear(hidden_size * 2, num_classes)
```

---

## üìã PIPELINE UNIFICADO PROPOSTO

### 1. Preprocessing (Temporal Windows)

#### Sleep-EDF:
```python
# Criar sleep_edf_temporal.py
def preprocess_sleep_edf_temporal(
    data_dir: str,
    window_size: int = 3000,  # 30s √ó 100 Hz
    overlap: float = 0.0,      # Sem overlap (√©pocas sequenciais)
    normalize: str = 'per-channel-zscore'
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Output shape: (n_windows, 4, 3000)
    Channels: [EEG_Fpz-Cz, EEG_Pz-Oz, EOG_H, EOG_V]
    """
```

#### WESAD:
```python
# Manter processamento atual
# Output shape: (n_windows, 14, 1920)
# J√° tem overlap de 50% e normaliza√ß√£o per-channel
```

### 2. Model Architecture (Unificado)

```python
class BaselineLSTM(nn.Module):
    """
    Arquitetura unificada para ambos os datasets
    Compat√≠vel com: Baseline, DP, FL, DP+FL
    """
    def __init__(self, input_channels, hidden_size, num_classes):
        super().__init__()
        
        # Normaliza√ß√£o per-channel (camada)
        self.channel_norm = nn.LayerNorm(input_channels)
        
        # LSTM Bidirectional
        self.lstm = nn.LSTM(
            input_size=input_channels,
            hidden_size=hidden_size,
            num_layers=2,
            batch_first=True,
            bidirectional=True,
            dropout=0.3
        )
        
        # Classifica√ß√£o
        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)
        self.dropout = nn.Dropout(0.3)
        self.fc2 = nn.Linear(hidden_size, num_classes)
    
    def forward(self, x):
        # x: (batch, channels, timesteps)
        x = x.permute(0, 2, 1)  # (batch, timesteps, channels)
        x = self.channel_norm(x)
        lstm_out, (hn, _) = self.lstm(x)
        x = torch.cat([hn[-2], hn[-1]], dim=1)  # Concat bidirectional
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x
```

### 3. Training Loop (Unificado)

```python
def train_baseline_unified(
    dataset_name: str,  # 'sleep-edf' ou 'wesad'
    model_config: dict,
    train_config: dict
):
    """
    Loop de treino unificado para ambos os datasets
    Garante mesma base para compara√ß√£o com DP/FL
    """
    
    # Configura√ß√µes comuns
    - Optimizer: Adam(lr=0.001, weight_decay=1e-4)
    - Loss: CrossEntropyLoss com class_weights
    - Scheduler: ReduceLROnPlateau
    - Early Stopping: patience=8
    - Batch size: 64
    - Max epochs: 100
```

---

## üéØ HYPERPAR√ÇMETROS RECOMENDADOS

### Sleep-EDF:
```python
config = {
    'input_channels': 4,      # EEG + EOG
    'hidden_size': 128,       # LSTM hidden units
    'num_layers': 2,          # LSTM layers
    'num_classes': 5,         # W, N1, N2, N3, R
    'dropout': 0.3,
    'learning_rate': 0.001,
    'batch_size': 64,
    'class_weights': True,    # Para desbalanceamento
}
```

### WESAD:
```python
config = {
    'input_channels': 14,     # Sinais fisiol√≥gicos
    'hidden_size': 128,       # Manter igual ao Sleep-EDF
    'num_layers': 2,          # Manter igual
    'num_classes': 2,         # non-stress, stress
    'dropout': 0.3,
    'learning_rate': 0.001,
    'batch_size': 64,
    'class_weights': True,    # Para desbalanceamento
}
```

---

## üîß COMPATIBILIDADE COM DP/FL

### Differential Privacy (DP):
```python
# Adiciona ru√≠do aos gradientes durante treino
# COMPAT√çVEL com temporal windows ‚úÖ
# Funciona sobre os dados raw/normalizados

from opacus import PrivacyEngine

privacy_engine = PrivacyEngine()
model, optimizer, train_loader = privacy_engine.make_private(
    module=model,
    optimizer=optimizer,
    data_loader=train_loader,
    noise_multiplier=1.0,
    max_grad_norm=1.0,
)
```

### Federated Learning (FL):
```python
# Treino distribu√≠do por sujeitos
# COMPAT√çVEL com subject-wise split ‚úÖ
# WESAD j√° tem split adequado

# Sleep-EDF precisa adaptar para subject-wise:
# - Identificar sujeitos por arquivo
# - Criar splits por sujeito (LOSO-style)
```

### DP + FL:
```python
# Combina ambas as t√©cnicas
# COMPAT√çVEL com setup proposto ‚úÖ
# Cada cliente (sujeito) treina com DP local
```

---

## üìä M√âTRICAS DE AVALIA√á√ÉO (Unificadas)

```python
metrics = {
    'accuracy': accuracy_score(y_true, y_pred),
    'precision': precision_score(y_true, y_pred, average='weighted'),
    'recall': recall_score(y_true, y_pred, average='weighted'),
    'f1_score': f1_score(y_true, y_pred, average='weighted'),
    'confusion_matrix': confusion_matrix(y_true, y_pred),
    
    # Espec√≠ficas para paper
    'per_class_f1': f1_score(y_true, y_pred, average=None),
    'training_time': time_elapsed,
    'inference_time': avg_inference_time,
}
```

---

## üöÄ PLANO DE IMPLEMENTA√á√ÉO

### Fase 1: Preprocessing Unificado
1. ‚úÖ Criar `sleep_edf_temporal.py` (janelas temporais)
2. ‚úÖ Adaptar `wesad.py` para compatibilidade
3. ‚úÖ Garantir mesma normaliza√ß√£o (per-channel z-score)
4. ‚úÖ Subject-wise split para ambos

### Fase 2: Baseline Unificado
1. ‚úÖ Criar `train_baseline_unified.py`
2. ‚úÖ Arquitetura LSTM comum
3. ‚úÖ Hyperpar√¢metros alinhados
4. ‚úÖ Treino e valida√ß√£o

### Fase 3: Extens√µes
1. ‚è≥ DP: Adicionar Opacus
2. ‚è≥ FL: Adicionar Flower
3. ‚è≥ DP+FL: Combinar ambos

### Fase 4: Experimentos
1. ‚è≥ Baseline puro
2. ‚è≥ Baseline + DP (v√°rios Œµ)
3. ‚è≥ Baseline + FL (v√°rios clientes)
4. ‚è≥ Baseline + DP + FL

---

## üìù TABELA COMPARATIVA PARA PAPER

| Method | Sleep-EDF Acc | Sleep-EDF F1 | WESAD Acc | WESAD F1 | Overhead |
|--------|---------------|--------------|-----------|----------|----------|
| Baseline | TBD | TBD | TBD | TBD | 1.0x |
| + DP (Œµ=10) | TBD | TBD | TBD | TBD | ~1.2x |
| + DP (Œµ=5) | TBD | TBD | TBD | TBD | ~1.2x |
| + DP (Œµ=1) | TBD | TBD | TBD | TBD | ~1.2x |
| + FL (3 clients) | TBD | TBD | TBD | TBD | ~2.0x |
| + FL (5 clients) | TBD | TBD | TBD | TBD | ~3.0x |
| + DP+FL | TBD | TBD | TBD | TBD | ~2.5x |

---

## üéØ CONCLUS√ÉO

### Recomenda√ß√£o Final:

**UNIFORMIZAR para Temporal Windows com LSTM Bidirectional**

**Raz√µes:**
1. ‚úÖ Compat√≠vel com TODAS as t√©cnicas (Baseline, DP, FL, DP+FL)
2. ‚úÖ Preserva informa√ß√£o temporal completa
3. ‚úÖ Permite compara√ß√£o justa entre datasets
4. ‚úÖ Mesma arquitetura = mesma complexidade
5. ‚úÖ Adequado para paper cient√≠fico rigoroso

**Pr√≥ximos Passos:**
1. Criar `sleep_edf_temporal.py` para janelas temporais
2. Criar `train_baseline_unified.py` com arquitetura comum
3. Validar resultados do baseline
4. Implementar DP/FL sobre baseline validado


