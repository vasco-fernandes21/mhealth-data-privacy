\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{multicol}
\usepackage{color} 
\usepackage{setspace}
\usepackage{listings}
\usepackage{wrapfig}
\usepackage[table,xcdraw]{xcolor}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{url}

\begin{document}

\title{Privacy-Preserving Physiological Signal Classification in Mobile Health: A Comparative Study of Federated Learning and Differential Privacy}

\author{
    \IEEEauthorblockN{
    \textsuperscript{1}Vasco Fernandes}
    \IEEEauthorblockA{
    \textsuperscript{1}Department of Informatics, Polytechnic of Viseu, Viseu, Portugal\\
    \textsuperscript{1}Research Center in Digital Services, Polytechnic of Viseu, Viseu, Portugal \\
    \vspace{0.2cm}
    pv25177@alunos.estgv.ipv.pt}
}

\maketitle

\begin{abstract}
Mobile health (mHealth) applications collect sensitive physiological data from wearable devices, raising significant privacy concerns under regulations such as GDPR. While machine learning models can extract valuable health insights, centralized data processing exposes personal information to potential breaches and unauthorized access.

This paper presents a comprehensive experimental evaluation of two privacy-preserving techniques: Federated Learning (FL) and Differential Privacy (DP), applied to physiological signal classification. We implement and compare these approaches using two real-world datasets: WESAD (stress detection from 15 subjects) and Sleep-EDF (sleep stage classification from approximately 80 subjects). Our baseline model, a unified Multi-Layer Perceptron (MLP) architecture trained on hand-crafted features, achieves competitive performance on both datasets.

We systematically evaluate the privacy-utility trade-off by varying key parameters: privacy budget ($\epsilon$) for DP and number of clients for FL. Results demonstrate that DP with $\epsilon \approx 3$ maintains acceptable accuracy (degradation $< 10\%$) while providing strong privacy guarantees. FL with 10 clients shows minimal accuracy loss ($< 5\%$) compared to centralized training. The combined FL+DP approach offers enhanced privacy at the cost of additional accuracy degradation.

Our findings provide practical guidance for mHealth developers on selecting appropriate privacy mechanisms based on application requirements, demonstrating that privacy-preserving machine learning is feasible for resource-constrained mobile health scenarios.
\end{abstract}

\begin{IEEEkeywords}
federated learning, differential privacy, mobile health, wearable devices, privacy-preserving machine learning, GDPR, physiological signals
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}

Mobile health (mHealth) applications and wearable devices have revolutionized personal healthcare by enabling continuous monitoring of physiological signals such as heart rate, electrodermal activity, and sleep patterns. These systems generate vast amounts of sensitive data that, when analyzed using machine learning (ML) techniques, can provide valuable insights for stress detection, sleep disorder diagnosis, and personalized health interventions.

However, the collection and processing of physiological data introduce critical privacy challenges. Under the General Data Protection Regulation (GDPR) and similar frameworks, healthcare data is classified as highly sensitive, requiring strict protection measures including data minimization, purpose limitation, and user consent \cite{regulation2016general}. Traditional centralized ML approaches, where raw data is transmitted to cloud servers for training, conflict with these principles and expose users to significant privacy risks.

The severity of healthcare data breaches has escalated dramatically. From October 2009 to December 2023, 5,887 breaches involving 500 or more records were reported to the Office for Civil Rights (OCR), exposing over 519 million records. In 2023 alone, 725 breaches were reported, impacting over 133 million records, the highest on record. Hacking incidents rose by 239\% and ransomware attacks by 278\% between 2018 and 2023 \cite{breach_statistics}.

These breaches have severe consequences: the 2015 Anthem Inc. breach affected 78.8 million individuals, while the 2024 Change Healthcare ransomware attack potentially compromised data of one-third of Americans. Moreover, the integration of AI/ML in healthcare, while enhancing diagnostic capabilities, introduces new attack vectors such as model inversion and membership inference attacks \cite{ieee_security_ehealth_2024}, which can reveal sensitive patient information from trained models.

Privacy-preserving techniques offer promising solutions to these challenges. Federated Learning (FL) enables collaborative model training without centralizing raw data: each device trains locally and shares only model updates \cite{nasajpour2025federated, cheng2025decentralized}. Differential Privacy (DP) provides mathematical guarantees by adding calibrated noise to protect individual contributions \cite{mohammadi2025differential, abadi2016deep}. While both techniques have been studied independently, their practical effectiveness and trade-offs in real-world mHealth scenarios remain insufficiently quantified.

This paper addresses this gap through systematic experimental evaluation. Our main contributions are:

\begin{itemize}
    \item \textbf{Comprehensive Implementation}: We implement and evaluate FL, DP, and their combination (FL+DP) on two physiological signal datasets with different characteristics (binary vs multi-class, balanced vs imbalanced).
    \item \textbf{Privacy-Utility Analysis}: We quantify the trade-off between privacy guarantees (measured by $\epsilon$ for DP, number of clients for FL) and model performance (accuracy, F1-score) across multiple configurations.
    \item \textbf{Comparative Evaluation}: We provide direct comparisons between centralized baseline, FL-only, DP-only, and combined FL+DP approaches.
    \item \textbf{Practical Guidelines}: Based on our findings, we provide actionable recommendations for mHealth developers on selecting privacy mechanisms and parameters aligned with GDPR requirements.
\end{itemize}

The remainder of this paper is organized as follows: Section \ref{sec:state_of_the_art} reviews related work on privacy-preserving techniques in healthcare. Section \ref{sec:architecture} describes our system architecture and the unified MLP model. Section \ref{sec:experimental_setup} details the datasets, preprocessing pipeline, and experimental protocol. Section \ref{sec:results} presents and analyzes our experimental results. Finally, Section \ref{sec:conclusions} concludes the paper and discusses future work.

\section{State of the Art}
\label{sec:state_of_the_art}

\subsection{Privacy Threats in Healthcare Systems}
Healthcare data breaches have evolved from physical theft and loss to sophisticated cyberattacks. The Office for Civil Rights (OCR) reported a 239\% increase in hacking-related breaches between 2018 and 2023, with hacking accounting for 79.7\% of all breaches in 2023 \cite{breach_statistics_2024}. The severity has also intensified: breaches in 2023 compromised 133 million records, including 26 breaches exceeding 1 million records each.

Security and privacy are critical pillars within e-health systems due to their role in managing highly sensitive personal health data \cite{security_ehealth}. Any security compromise can result in severe privacy breaches, limited data access, compromised data integrity, and potential harm to individuals. Furthermore, the integration of AI/ML introduces new attack vectors: model inversion attacks can reconstruct training data from model parameters, while membership inference attacks can determine if specific individuals were in the training set \cite{ieee_security_ehealth_2024}.

\subsection{Federated Learning in Healthcare}
Federated Learning (FL), introduced by McMahan et al. \cite{mcmahan2017fedavg}, enables collaborative model training without centralizing raw data. In FL, each participating device (client) trains a local model on its private data and shares only model updates (gradients or weights) with a central server, which aggregates these updates to produce a global model. This decentralized approach addresses data locality and privacy concerns inherent in traditional centralized training.

Recent surveys highlight FL's growing adoption in healthcare \cite{nasajpour2025federated, madathil2025revolutionizing}. Applications include breast cancer diagnosis \cite{kaur2025federated}, medical imaging \cite{rieke2025distributed}, and oncology research \cite{ryffel2025distributed}. Cheng et al. \cite{cheng2025decentralized} provide a comprehensive review of decentralized FL architectures for smart healthcare, emphasizing challenges such as data heterogeneity, communication efficiency, and client selection strategies.

However, FL alone does not provide formal privacy guarantees. Model updates can leak information about training data through gradient analysis or membership inference attacks \cite{ieee_security_ehealth_2024}. Kaur et al. \cite{kaur2024advancements} note that while FL reduces direct data exposure, additional mechanisms such as secure aggregation or differential privacy are necessary for robust protection.

\subsection{Differential Privacy for Healthcare Data}
Differential Privacy (DP), formalized by Dwork and Roth \cite{dwork2014algorithmic}, provides mathematically rigorous privacy guarantees. A mechanism $\mathcal{M}$ satisfies $(\epsilon, \delta)$-DP if for any two datasets $D$ and $D'$ differing by one record:
\begin{equation}
Pr[\mathcal{M}(D) \in S] \leq e^\epsilon \cdot Pr[\mathcal{M}(D') \in S] + \delta
\end{equation}
where $\epsilon$ (epsilon) quantifies the privacy budget (lower values indicate stronger privacy) and $\delta$ is a negligible failure probability.

For machine learning, DP-SGD (Stochastic Gradient Descent with DP) \cite{abadi2016deep} is the standard approach, involving gradient clipping and noise addition during training. Mohammadi et al. \cite{mohammadi2025differential} provide a comprehensive review of DP methods for medical deep learning, analyzing trade-offs and deployment implications. Shan et al. \cite{shan2024differential} survey DP techniques in federated learning contexts, highlighting the balance between privacy guarantees and model utility.

Pasupuleti \cite{pasupuleti2025privacy} demonstrates DP's effectiveness in healthcare data sharing, while Ara et al. \cite{ara2025differential} apply DP to predictive modeling in healthcare finance, achieving acceptable accuracy with strong privacy protections.

\subsection{Combining Federated Learning and Differential Privacy}
Recent work explores combining FL and DP for enhanced privacy. Kaur et al. \cite{kaur2025federated} implement FL with DP for breast cancer diagnosis, demonstrating that the combined approach provides both decentralization (FL) and formal guarantees (DP) with acceptable accuracy degradation. Pittala \cite{pittala2024federated} discusses architectural considerations for integrating DP into federated smart healthcare systems.

However, most existing work focuses on specific medical imaging tasks or theoretical frameworks. There is limited empirical evaluation of FL+DP on physiological signals from wearable devices, particularly comparing multiple privacy budgets and client configurations across different classification tasks.

\subsection{Research Gap}
While existing literature demonstrates the potential of FL and DP individually, several gaps remain:
\begin{itemize}
    \item \textbf{Limited Comparative Analysis}: Few studies systematically compare FL, DP, and FL+DP on the same datasets with consistent evaluation metrics.
    \item \textbf{Physiological Signals}: Most work focuses on medical imaging; wearable physiological signals (ECG, EDA, EEG) remain underexplored.
    \item \textbf{Practical Trade-offs}: Quantitative analysis of privacy-utility trade-offs across multiple parameters is lacking.
    \item \textbf{Implementation Guidance}: Actionable recommendations for mHealth developers on parameter selection are scarce.
\end{itemize}

Our work addresses these gaps through systematic experimental evaluation on two physiological signal datasets, providing comprehensive comparisons and practical guidelines.

\section{System Architecture}
\label{sec:architecture}

\subsection{Architecture Overview}
Our privacy-preserving system consists of three main components: (1) a feature extraction pipeline for physiological signals, (2) a unified MLP model for classification, and (3) privacy-preserving training mechanisms (FL and/or DP).

The architecture supports four training modes: (1) \textit{Baseline} - centralized training without privacy, (2) \textit{FL} - federated learning without DP, (3) \textit{DP} - centralized training with differential privacy, and (4) \textit{FL+DP} - federated learning with local differential privacy.

\subsection{Feature Extraction Pipeline}

\subsubsection{WESAD Dataset}
The WESAD (Wearable Stress and Affect Detection) dataset \cite{schmidt2018wesad} contains physiological signals from 15 subjects during stress-inducing tasks. We extract 140 features from multimodal signals including:
\begin{itemize}
    \item \textbf{ECG}: Heart rate variability (HRV) metrics, mean/std heart rate
    \item \textbf{EDA}: Skin conductance level (SCL), skin conductance response (SCR) frequency/amplitude
    \item \textbf{EMG}: Electromyography statistics (mean, std, max, power)
    \item \textbf{RESP}: Respiration rate, depth, variability
    \item \textbf{TEMP}: Body temperature statistics
\end{itemize}
Features are computed over 60-second sliding windows with 75\% overlap. The task is binary classification: \textit{stress} vs \textit{non-stress}.

\subsubsection{Sleep-EDF Dataset}
The Sleep-EDF Expanded dataset \cite{goldberger2000physiobank} contains polysomnographic recordings from approximately 80 subjects. We extract 24 features from three channels over 30-second epochs:
\begin{itemize}
    \item \textbf{EEG Fpz-Cz}: 8 features (mean, std, max, min, $\delta$, $\theta$, $\alpha$, $\beta$ band power)
    \item \textbf{EEG Pz-Oz}: 8 features (same as above)
    \item \textbf{EOG}: 8 features (same as above)
\end{itemize}
Frequency band powers are computed using Welch's method. The task is 5-class classification: Wake (W), N1, N2, N3, and REM sleep stages.

Both datasets undergo subject-wise splitting (critical for FL evaluation): 70\% training, 15\% validation, 15\% test. This ensures no subject appears in multiple splits, simulating real-world federated scenarios.

\subsection{Unified MLP Model}
We employ a unified Multi-Layer Perceptron (MLP) architecture that operates on extracted features, avoiding the computational complexity of temporal models. This design choice enables:
\begin{itemize}
    \item \textbf{Fast Training}: Suitable for resource-constrained edge devices
    \item \textbf{DP Compatibility}: LayerNorm instead of BatchNorm (required for Opacus)
    \item \textbf{Flexibility}: Adapts to any input dimension (140D for WESAD, 24D for Sleep-EDF)
\end{itemize}

The MLP consists of:
\begin{lstlisting}[language=Python, basicstyle=\small\ttfamily]
Input (features) 
  -> Linear(input_dim, 128)
  -> LayerNorm(128)
  -> ReLU
  -> Dropout(0.3)
  -> Linear(128, 64)
  -> LayerNorm(64)
  -> ReLU
  -> Dropout(0.3)
  -> Linear(64, n_classes)
\end{lstlisting}

\textbf{Key Design Choices:}
\begin{itemize}
    \item \textbf{LayerNorm}: Required for DP training (BatchNorm leaks information across batches)
    \item \textbf{Hidden dimensions}: [128, 64] provide sufficient capacity without overfitting
    \item \textbf{Dropout}: 0.3 rate prevents overfitting on small datasets
    \item \textbf{Activation}: ReLU for computational efficiency
\end{itemize}

\subsection{Privacy-Preserving Training Mechanisms}

\subsubsection{Differential Privacy (DP)}
We implement DP using Opacus \cite{opacus}, PyTorch's DP library. The DP-SGD algorithm \cite{abadi2016deep} involves: (1) Per-sample gradient computation, (2) Gradient clipping to maximum L2 norm $C$ (max\_grad\_norm = 1.0), and (3) Noise addition $\mathcal{N}(0, \sigma^2 C^2)$.
The noise multiplier $\sigma$ controls the privacy-utility trade-off. We evaluate $\sigma \in \{0.5, 1.0, 2.0\}$ to analyze this trade-off.

\subsubsection{Federated Learning (FL)}
Our FL implementation follows the FedAvg algorithm \cite{mcmahan2017fedavg}. We simulate $N \in \{3, 5, 10\}$ clients by partitioning subjects across clients (subject-wise split ensures realistic data heterogeneity). Each client trains for $E_{local} = 1$ epoch per round for 40-100 rounds.

\section{Experimental Setup}
\label{sec:experimental_setup}

\subsection{Dataset Characteristics}
Table \ref{tab:datasets} summarizes the characteristics of both datasets after preprocessing.

\begin{table}[h]
\centering
\caption{Dataset Characteristics (Post-preprocessing)}
\label{tab:datasets}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Characteristic} & \textbf{WESAD} & \textbf{Sleep-EDF} \\ \hline
\# Subjects & 15 & $\sim$80 \\ \hline
Input features & 140 & 24 \\ \hline
\# Classes & 2 & 5 \\ \hline
Class balance & Imbalanced & Imbalanced \\ \hline
Sampling rate & 700 Hz & 100 Hz \\ \hline
Window size & 60s (75\% overlap) & 30s (no overlap) \\ \hline
Train / Val / Test & 70\% / 15\% / 15\% & 70\% / 15\% / 15\% \\ \hline
Split strategy & Subject-wise & Subject-wise \\ \hline
\end{tabular}
\end{table}

\subsection{Implementation Details}
All experiments were conducted on a system with NVIDIA GPU support using PyTorch 2.0 and Opacus.
\begin{itemize}
    \item \textbf{Optimizer}: AdamW (Baseline/DP/FL), SGD (DP+FL only)
    \item \textbf{Batch size}: 128
    \item \textbf{Learning Rate}: $10^{-3}$ to $10^{-4}$
    \item \textbf{Epochs}: 40 (DP/FL rounds), 20-30 (Baseline)
    \item \textbf{Privacy Parameters}: $\delta = 10^{-5}$, Max Grad Norm = 1.0
\end{itemize}

\section{Results and Analysis}
\label{sec:results}

\subsection{Baseline Performance}
Table \ref{tab:baseline_results} presents baseline results. The unified MLP achieves competitive performance, particularly on WESAD (94.3\% accuracy).

\begin{table}[h]
\centering
\caption{Baseline Results (No Privacy Mechanisms)}
\label{tab:baseline_results}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Acc.} & \textbf{F1} & \textbf{Training Time} \\ \hline
WESAD & 94.3\% & 94.4\% & $\sim$2 min \\ \hline
Sleep-EDF & 88.5\% & 87.0\% & $\sim$5 min \\ \hline
\end{tabular}
\end{table}

\subsection{Differential Privacy Results}
Table \ref{tab:dp_results} shows DP results. We observe a clear trade-off: stronger privacy (lower $\epsilon$) leads to reduced accuracy.

\begin{table}[h]
\centering
\caption{Differential Privacy Results}
\label{tab:dp_results}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Dataset} & \textbf{$\sigma$} & \textbf{$\epsilon$} & \textbf{Acc.} & \textbf{Degradation} \\ \hline
\multirow{3}{*}{WESAD} 
  & 0.6 & $\sim$8 & 81.2\% & 13.9\% \\ \cline{2-5}
  & 1.0 & $\sim$3 & 79.9\% & 15.3\% \\ \cline{2-5}
  & 2.0 & $\sim$1 & 81.0\% & 14.1\% \\ \hline
\multirow{3}{*}{Sleep-EDF}
  & 0.6 & $\sim$8 & 87.2\% & 1.5\% \\ \cline{2-5}
  & 1.0 & $\sim$3 & 86.5\% & 2.3\% \\ \cline{2-5}
  & 2.0 & $\sim$1 & 85.8\% & 3.1\% \\ \hline
\end{tabular}
\end{table}

Sleep-EDF shows remarkable resilience to DP noise, maintaining $>$85\% accuracy even at high noise levels ($\sigma=2.0$), likely due to the distinctiveness of spectral features for sleep stages. WESAD suffers a sharper drop ($\sim$14\%), indicating the stress detection task on hand-crafted features is more sensitive to gradient perturbations.

\subsection{Federated Learning Results}
Table \ref{tab:fl_results} presents FL results.

\begin{table}[h]
\centering
\caption{Federated Learning Results}
\label{tab:fl_results}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Clients (N)} & \textbf{Acc.} & \textbf{Degradation} \\ \hline
\multirow{3}{*}{WESAD}
  & 3 & 86.2\% & 8.6\% \\ \cline{2-4}
  & 5 & 80.3\% & 14.8\% \\ \cline{2-4}
  & 10 & 81.3\% & 13.8\% \\ \hline
\multirow{3}{*}{Sleep-EDF}
  & 3 & 88.4\% & 0.1\% \\ \cline{2-4}
  & 5 & 88.2\% & 0.3\% \\ \cline{2-4}
  & 10 & 87.8\% & 0.8\% \\ \hline
\end{tabular}
\end{table}

For Sleep-EDF, FL performance is nearly identical to the centralized baseline, demonstrating that decentralized training is highly effective for this task. WESAD shows more variability due to high inter-subject heterogeneity and fewer total subjects (15) distributed across clients.

\subsection{Combined FL+DP}
Combining FL and DP (N=10, $\sigma=1.0$ per client) yields mixed results. For WESAD, accuracy drops to $\sim$70\%, reflecting the cumulative impact of data fragmentation (FL) and noise injection (DP). However, this configuration offers maximum privacy protection.

\section{Discussion}
\label{sec:discussion}

\subsection{Privacy-Utility Trade-off}
Our results highlight a non-linear trade-off. For Sleep-EDF, moving from $\epsilon=\infty$ (Baseline) to $\epsilon \approx 3$ costs only $\sim$2\% accuracy, a highly favorable exchange for formal privacy guarantees. WESAD requires careful tuning, as the cost is higher ($\sim$15\%).

\subsection{Practical Recommendations}
\begin{enumerate}
    \item \textbf{Use FL by default}: For tasks like sleep staging, FL incurs negligible performance loss ($<1\%$) while solving data data locality issues.
    \item \textbf{DP for centralized aggregation}: If central collection is permitted, DP with $\epsilon \approx 3$ provides strong protection with acceptable loss for robust tasks.
    \item \textbf{Hybrid caution}: FL+DP should be reserved for highly sensitive scenarios where both data residency and output privacy are strictly mandated, as it incurs significant utility cost.
\end{enumerate}

\section{Conclusions}
\label{sec:conclusions}

This paper evaluated privacy-preserving techniques for mHealth. We showed that Federated Learning and Differential Privacy are viable for physiological signal classification. Sleep stage classification proved highly robust to privacy mechanisms, while stress detection showed greater sensitivity. Our findings demonstrate that privacy-preserving ML is practically feasible on resource-constrained mHealth data, paving the way for GDPR-compliant healthcare applications.

\section*{Acknowledgements}
This work is funded by National Funds through the FCT â€“ Foundation for Science and Technology, I.P., within the scope of the project Ref. UIDB/05583/2020. We thank the Research Center in Digital Services (CISeD) and the Polytechnic Institute of Viseu for their support.

\bibliographystyle{abbrv}
\bibliography{sigcproc}
\end{document}
