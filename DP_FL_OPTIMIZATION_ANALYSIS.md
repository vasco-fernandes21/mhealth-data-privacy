# üöÄ An√°lise e Otimiza√ß√£o: Differential Privacy & Federated Learning

## üìä AN√ÅLISE DA ESTRUTURA DOS DADOS PROCESSADOS

### Sleep-EDF
- **Shape**: (313,922 train, 66,753 val, 72,330 test) √ó 24 features
- **Formato**: Dados j√° extra√≠dos (features temporais + frequ√™ncia)
- **Tipo**: Dados tabulares 2D ‚Üí Requer windowing para LSTM

### WESAD  
- **Shape**: (715 train, 237 val, 237 test) √ó 14 channels √ó 1920 timesteps
- **Formato**: Janelas temporais pr√©-processadas
- **Tipo**: Dados 3D prontos para LSTM

---

## üîç PROBLEMAS IDENTIFICADOS

### 1. **SLEEP-EDF: Windowing Redundante e Lento**
```python
# ‚ùå PROBLEMA: Cria windows em CADA √©poca de treinamento
def create_windows(X, y, window_size):
    n_windows = n_samples - window_size + 1
    for i in range(n_windows):  # Loop Python lento!
        X_windows[i] = X[i:i+window_size]
        y_windows[i] = y[i+window_size-1]
```

**Impacto**: 
- ~300k windows criadas por √©poca
- Loop Python n√£o vetorizado
- Tempo desperdi√ßado: ~5-10 segundos por √©poca

### 2. **DP: DataLoader N√£o Otimizado**
```python
# ‚ùå PROBLEMA: Sem workers paralelos, sem pin_memory
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
```

**Impacto**:
- I/O s√≠ncrono (bloqueante)
- Sem prefetching de dados
- GPU fica ociosa esperando dados

### 3. **FL: Cria√ß√£o de Windows Repetida por Cliente**
```python
# ‚ùå PROBLEMA: Cada cliente recria windows
def client_fn(cid: str):
    X_client, y_client = client_datasets[int(cid)]
    # Windows criadas aqui = N_clients √ó tempo
```

**Impacto**:
- 3-5 clientes √ó tempo de windowing
- Mem√≥ria duplicada
- Simula√ß√£o mais lenta

### 4. **DP: Augmentation em Tempo de Treino**
```python
# ‚ùå PROBLEMA: Augmentation determin√≠stica a cada treino
def _augment_temporal(X, noise_std=0.01, max_time_shift=8):
    # Loop Python para time shifts
    for i in range(n_samples):
        if shift != 0:
            X_aug[i] = np.pad(...)  # Opera√ß√£o lenta
```

**Impacto**:
- Augmentation poderia ser pr√©-calculada
- Loops Python lentos
- Overhead em cada √©poca

### 5. **FL: Sem Cache de Modelo Base**
```python
# ‚ùå PROBLEMA: Modelo criado N vezes
def client_fn(cid: str):
    model = SimpleLSTM(...).to(device)  # Nova inicializa√ß√£o
```

**Impacto**:
- Inicializa√ß√£o repetida
- Overhead de mem√≥ria
- Tempo de setup por round

---

## ‚úÖ OTIMIZA√á√ïES PROPOSTAS

### üéØ **1. PR√â-PROCESSAR WINDOWS UMA VEZ (Sleep-EDF)**

#### Antes:
```python
# Dados salvos: X_train.npy (313922, 24)
# Windows criadas em runtime
```

#### Depois:
```python
# Salvar dados j√° em formato windowed
# X_train_windows.npy (313913, 10, 24)  ‚Üê Pronto para LSTM!
```

**Implementa√ß√£o**:
```python
def save_windowed_data(data_dir: str, window_size: int = 10):
    """Pr√©-processa e salva dados em formato windowed"""
    X_train = np.load(f"{data_dir}/X_train.npy")
    y_train = np.load(f"{data_dir}/y_train.npy")
    
    # Vetoriza√ß√£o usando view + stride tricks
    from numpy.lib.stride_tricks import sliding_window_view
    X_windows = sliding_window_view(X_train, window_size, axis=0)
    X_windows = X_windows.transpose(0, 2, 1)  # (n_windows, window, features)
    y_windows = y_train[window_size-1:]  # Labels alinhados
    
    np.save(f"{data_dir}/X_train_windows.npy", X_windows)
    np.save(f"{data_dir}/y_train_windows.npy", y_windows)
```

**Ganho Esperado**: 
- ‚ö° **5-10s por √©poca** eliminados
- üìâ Redu√ß√£o de 30-40% no tempo total de treino
- üîÑ Uma √∫nica vez no pr√©-processamento

---

### üéØ **2. OTIMIZAR DATALOADERS (DP & FL)**

#### Antes:
```python
train_loader = DataLoader(dataset, batch_size=64, shuffle=True)
```

#### Depois:
```python
train_loader = DataLoader(
    dataset, 
    batch_size=64, 
    shuffle=True,
    num_workers=4,           # ‚úÖ Paralelo
    pin_memory=True,         # ‚úÖ GPU transfer r√°pido
    persistent_workers=True, # ‚úÖ Reutiliza workers
    prefetch_factor=2,       # ‚úÖ Prefetch batches
    drop_last=True           # ‚úÖ DP requer batch fixo
)
```

**Ganho Esperado**:
- ‚ö° **20-30% mais r√°pido** por √©poca
- üéØ GPU utiliza√ß√£o aumenta de ~60% ‚Üí ~90%
- üìä Throughput aumenta ~1.5√ó

---

### üéØ **3. PR√â-CALCULAR AUGMENTATION (DP)**

#### Antes:
```python
# Augmentation em cada √©poca
X_train_aug = _augment_temporal(X_train, seed=SEED)
```

#### Depois:
```python
# Salvar augmented data uma vez
def save_augmented_data(data_dir: str, n_augmentations: int = 2):
    X_train = np.load(f"{data_dir}/X_train.npy")
    y_train = np.load(f"{data_dir}/y_train.npy")
    
    X_aug_list = [X_train]
    y_aug_list = [y_train]
    
    for i in range(n_augmentations):
        X_aug = _augment_temporal(X_train, seed=SEED+i)
        X_aug_list.append(X_aug)
        y_aug_list.append(y_train)
    
    X_all = np.concatenate(X_aug_list, axis=0)
    y_all = np.concatenate(y_aug_list, axis=0)
    
    np.save(f"{data_dir}/X_train_augmented.npy", X_all)
    np.save(f"{data_dir}/y_train_augmented.npy", y_all)
```

**Ganho Esperado**:
- ‚ö° **2-3s por √©poca** eliminados
- üîÑ Augmentation determin√≠stica garantida
- üíæ Trade-off: +2√ó storage

---

### üéØ **4. CACHE DE WINDOWED DATA (FL)**

#### Antes:
```python
def partition_data(X_train, y_train, num_clients):
    # Windows criadas por cliente
    client_datasets = []
    for i in range(num_clients):
        X_windows, y_windows = create_windows(X_client, y_client)
        client_datasets.append((X_windows, y_windows))
```

#### Depois:
```python
# Usar dados pr√©-windowed
X_train_windows = np.load("X_train_windows.npy")
y_train_windows = np.load("y_train_windows.npy")

def partition_windowed_data(X_windows, y_windows, num_clients):
    # Particionar dados j√° processados
    dataset_size = len(X_windows)
    client_size = dataset_size // num_clients
    indices = np.random.permutation(dataset_size)
    
    client_datasets = []
    for i in range(num_clients):
        start = i * client_size
        end = start + client_size if i < num_clients - 1 else dataset_size
        client_datasets.append((X_windows[indices[start:end]], 
                               y_windows[indices[start:end]]))
    return client_datasets
```

**Ganho Esperado**:
- ‚ö° **10-15s** eliminados por run
- üöÄ Inicializa√ß√£o de clientes ~5√ó mais r√°pida
- üíæ Mem√≥ria mais eficiente (sem duplica√ß√£o)

---

### üéØ **5. OTIMIZAR TRAINING LOOPS (DP)**

#### Antes:
```python
def train_one_epoch_dp(model, loader, criterion, optimizer, device, privacy_engine):
    for batch_idx, (batch_X, batch_y) in enumerate(loader):
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)
        
        # Impress√£o frequente (overhead)
        if (batch_idx + 1) % 3 == 0:
            print(f"Batch {batch_idx + 1}/{len(loader)} - Loss: {loss.item():.4f}")
```

#### Depois:
```python
def train_one_epoch_dp(model, loader, criterion, optimizer, device, privacy_engine):
    # Progress bar com tqdm (mais eficiente)
    pbar = tqdm(loader, desc="Training", leave=False)
    
    for batch_X, batch_y in pbar:
        batch_X, batch_y = batch_X.to(device, non_blocking=True)
        batch_y = batch_y.to(device, non_blocking=True)
        
        optimizer.zero_grad(set_to_none=True)  # ‚úÖ Mais r√°pido
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        
        # Update progress bar (sem overhead)
        pbar.set_postfix({'loss': loss.item()})
```

**Ganho Esperado**:
- ‚ö° **5-10%** mais r√°pido por √©poca
- üìä Melhor visualiza√ß√£o de progresso
- üéØ `non_blocking=True` + `set_to_none=True`

---

### üéØ **6. MIXED PRECISION TRAINING (DP & FL)**

```python
# Antes: FP32 padr√£o
model = SimpleLSTM(...).to(device)

# Depois: FP16/BF16 para acelera√ß√£o
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for batch_X, batch_y in loader:
    with autocast():  # ‚úÖ Opera√ß√µes em FP16
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

**Ganho Esperado**:
- ‚ö° **30-50%** mais r√°pido (se GPU suporta)
- üíæ 50% menos mem√≥ria GPU
- ‚ö†Ô∏è Requer GPU moderna (Volta+, M1/M2/M3)

---

### üéØ **7. COMPILA√á√ÉO DE MODELO (PyTorch 2.0+)**

```python
# Antes
model = SimpleLSTM(...).to(device)

# Depois: torch.compile
model = SimpleLSTM(...).to(device)
model = torch.compile(model, mode='reduce-overhead')  # ‚úÖ JIT compilation
```

**Ganho Esperado**:
- ‚ö° **10-20%** mais r√°pido
- üéØ Funciona bem com LSTMs
- ‚ö†Ô∏è Requer PyTorch 2.0+

---

## üìà ESTIMATIVA DE GANHOS TOTAIS

### **Sleep-EDF DP**
| Otimiza√ß√£o | Ganho | Impacto Cumulativo |
|------------|-------|-------------------|
| Pre-windowing | -5s/√©poca | **30-40%** |
| DataLoader otimizado | -20% tempo | **+20%** |
| Training loop | -5% | **+5%** |
| Mixed precision | -30% (GPU) | **+30%** |
| **TOTAL** | | **üöÄ 50-60% mais r√°pido** |

### **WESAD DP**
| Otimiza√ß√£o | Ganho | Impacto Cumulativo |
|------------|-------|-------------------|
| Pre-augmentation | -2s/√©poca | **15-20%** |
| DataLoader otimizado | -20% | **+20%** |
| Training loop | -5% | **+5%** |
| Mixed precision | -30% (GPU) | **+30%** |
| **TOTAL** | | **üöÄ 45-55% mais r√°pido** |

### **Federated Learning**
| Otimiza√ß√£o | Ganho | Impacto Cumulativo |
|------------|-------|-------------------|
| Pre-windowing (Sleep-EDF) | -10s setup | **20-25%** |
| DataLoader otimizado | -15% | **+15%** |
| Cached clients | -20% | **+20%** |
| **TOTAL** | | **üöÄ 40-50% mais r√°pido** |

---

## üõ†Ô∏è PLANO DE IMPLEMENTA√á√ÉO

### **Fase 1: Pr√©-processamento (Alta Prioridade)**
1. ‚úÖ Adicionar fun√ß√£o `save_windowed_data` em `preprocessing/sleep_edf.py`
2. ‚úÖ Adicionar fun√ß√£o `save_augmented_data` em `preprocessing/wesad.py`
3. ‚úÖ Executar pr√©-processamento uma vez
4. ‚úÖ Atualizar fun√ß√µes de carregamento

### **Fase 2: DataLoaders (M√©dia Prioridade)**
1. ‚úÖ Atualizar todos os DataLoaders com workers paralelos
2. ‚úÖ Adicionar `pin_memory` e `persistent_workers`
3. ‚úÖ Testar impacto na velocidade

### **Fase 3: Training Loops (M√©dia Prioridade)**
1. ‚úÖ Substituir prints por tqdm
2. ‚úÖ Adicionar `non_blocking=True`
3. ‚úÖ Usar `set_to_none=True` em `zero_grad()`

### **Fase 4: Advanced (Baixa Prioridade)**
1. ‚ö†Ô∏è Testar mixed precision (se GPU dispon√≠vel)
2. ‚ö†Ô∏è Testar torch.compile (PyTorch 2.0+)
3. ‚ö†Ô∏è Profiling detalhado com `torch.profiler`

---

## üìù RECOMENDA√á√ïES ESPEC√çFICAS

### **Para Differential Privacy**
- ‚úÖ **Cr√≠tico**: Pre-windowing (Sleep-EDF) e DataLoader otimizado
- ‚úÖ **Importante**: Pre-augmentation (WESAD)
- ‚ö†Ô∏è **Opcional**: Mixed precision (ganho depende de GPU)

### **Para Federated Learning**
- ‚úÖ **Cr√≠tico**: Pre-windowing (Sleep-EDF)
- ‚úÖ **Importante**: DataLoader com workers
- ‚ö†Ô∏è **Opcional**: Client caching (complexidade vs ganho)

### **Compatibilidade**
- ‚úÖ Todas otimiza√ß√µes Fase 1-3 s√£o compat√≠veis com Opacus (DP)
- ‚úÖ Flower (FL) suporta todas otimiza√ß√µes propostas
- ‚ö†Ô∏è Mixed precision: testar compatibilidade com Opacus
- ‚ö†Ô∏è torch.compile: pode ter issues com DPLSTM

---

## üéØ PR√ìXIMOS PASSOS

1. **Implementar Fase 1** (pr√©-processamento)
   - Modificar `preprocessing/sleep_edf.py`
   - Modificar `preprocessing/wesad.py`
   - Executar pr√©-processamento

2. **Atualizar Scripts de Treino**
   - Modificar DataLoaders em todos os scripts
   - Atualizar training loops com tqdm

3. **Benchmarking**
   - Medir tempo antes/depois
   - Comparar m√©tricas de performance
   - Documentar ganhos reais

4. **Valida√ß√£o**
   - Verificar que resultados s√£o reprodut√≠veis
   - Confirmar que DP/FL ainda funcionam corretamente
   - Testar com diferentes configura√ß√µes

---

## üìä M√âTRICAS DE SUCESSO

- ‚è±Ô∏è **Tempo de treino reduzido em 40-60%**
- üéØ **GPU utiliza√ß√£o > 85%**
- üîÑ **Reprodutibilidade mantida**
- ‚úÖ **M√©tricas de performance inalteradas**
- üìà **Throughput aumentado em 1.5-2√ó**


